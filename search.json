[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regressie",
    "section": "",
    "text": "Inleiding\nDit boek bevat door mij verzamelde notities over regressie en correlatie. Veel voorbeelden zijn afkomstig van internet of uit diverse boeken. Waar mogelijk is zoveel mogelijk gebruik gemaakt van tidyverse en het bijbehorende datatype tibble.\nHoewel regressie en correlatie vaak samen gebruikt worden, zijn het twee verschillende begrippen.\n\nRegressie heeft als doel om de soort relatie tussen \\(y\\) en \\(x\\) te bepalen. Bijvoorbeeld neemt \\(y\\) lineair toe als \\(x\\) toeneemt.\nCorrelatie meet de sterkte van een relatie. Is die sterk of juist zwak.\n\nRegressieanalyse bestaat uit een reeks machine learning-methoden waarmee je een continue uitkomstvariabele (\\(y\\)) kunt voorspellen op basis van de waarde van een of meerdere voorspellende variabelen (\\(x\\)).\nKort gezegd komt het er op neer om een wiskundige vergelijking te maken waarin \\(y\\) gedefinieerd wordt als functie van \\(x\\) variabelen.\n\n\\(y\\): afhankelijke (respons, uitkomst) variabele.\n\\(x\\): onafhankelijke (verklarende, voorspellende) variabele(n).\n\nVervolgens kan deze vergelijking worden gebruikt om de uitkomst \\(y\\) te voorspellen op basis van nieuwe waarden van de voorspellende variabelen \\(x\\).\n\n\n\n\n\n\nBelangrijk\n\n\n\nRegressie en correlatie verklaren de wijzigingen in de afhankelijke variabele, maar niet de oorzaak van deze wijzigingen. Dus niet of er een oorzakelijk verband is.\n\n\nDe volgende regressietechnieken komen aan bod\n\nLineaire regressie\nLogistische regresssie\n\n\n\n\n\n1. ‘Fisher’s Exact Test’, Wikipedia, (2022).\n\n\n2. ‘Unusual Observations - Outlier, Leverage, and Influential Points’, https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A., ‘Statistiek om mee te werken’, Third dr., Stenfert Kroese, (1991).\n\n\n4. Chouldechova, A., ‘Regression Diagnostic Plots’, https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, ‘R Linear Regression Tutorial: Lm Function in R with Code Examples’, https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., ‘Finding a Suitable Linear Model for Ozone Prediction’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M., ‘Interpreting Linear Prediction Models’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, ‘Beginners Guide to Regression Analysis and Plot Interpretations’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, ‘Practical Guide to Logistic Regression Analysis in R’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A., ‘Logistic Regression Essentials in R’, 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., ‘Machine Learning Essentials: Practical Guide in R’, 1st edition dr., (2018).\n\n\n12. University of Virginia, ‘Understanding Diagnostic Plots for Linear Regression Analysis | University of Virginia Library Research Data Services + Sciences’, https://data.library.virginia.edu/diagnostic-plots/."
  },
  {
    "objectID": "linmodel.html#data",
    "href": "linmodel.html#data",
    "title": "1  Lineair model",
    "section": "1.1 Data",
    "text": "1.1 Data\nDe dataset marketing, afkomstig uit package datarium, bevat de impact van drie advertentiemedia (youtube, facebook en newspaper) op de verkopen (sales). Het doel is het voorspellen van de verkopen op basis van de hoeveelheid geld (in duizenden dollars) die aan de drie advertentiemedia wordt uitgegeven.\nVariabelen\n\nuitkomstvariabele\n\nsales\n\nvoorspelvariabelen\n\nyoutube\nfacebook\nnewspaper\n\n\n\ndata(\"marketing\", package = \"datarium\")\nhead(marketing)\n\n  youtube facebook newspaper sales\n1   276.1     45.4      83.0 26.52\n2    53.4     47.2      54.1 12.48\n3    20.6     55.1      83.2 11.16\n4   181.8     49.6      70.2 22.20\n5   217.0     13.0      70.1 15.48\n6    10.4     58.7      90.0  8.64\n\n\nCorrelaties\nOm te zien of de aannames van het lineaire model geschikt zijn voor de beschikbare gegevens, worden de correlaties tussen de variabelen bekeken: een scatterplot en de correlatiecoëfficienten.\n\nplot(marketing)\ncor(marketing)\n\n          youtube facebook newspaper sales\nyoutube    1.0000   0.0548    0.0566 0.782\nfacebook   0.0548   1.0000    0.3541 0.576\nnewspaper  0.0566   0.3541    1.0000 0.228\nsales      0.7822   0.5762    0.2283 1.000\n\n\n\n\n\nFiguur 1.1: Correlaties tussen de variabelen.\n\n\n\n\nLineaire regressie veronderstelt een lineair verband tussen de uitkomstvariabele en de voorspelvariabele(n). De grafieken van sales tegen youtube en facebook tonen een stijgend lineair verband. Die tegen newspaper toont geen lineair verband. Vooralsnog wordt newspaper in het model meegenomen.\nHet doel is om een wiskundige formule te maken die sales definieert als functie van youtube, facebook en newspaper. Het regressiemodel wordt dan\n\\(sales = b_0 + b_1*youtube + b_2*facebook + b_3*newspaper\\)\nDe workflow wordt nu\n\nSplits de gegevens willekeurig in een trainingsset (80%) en een testset (20%).\nMaak een regressiemodel met de trainingsset. Beoordeel het model en pas het eventueel aan.\nMaak met het regressiemodel voorspellingen voor de testset en bereken de nauwkeurigheid van de voorspellingen."
  },
  {
    "objectID": "linmodel.html#splitsing-data",
    "href": "linmodel.html#splitsing-data",
    "title": "1  Lineair model",
    "section": "1.2 Splitsing data",
    "text": "1.2 Splitsing data\n\nset.seed(123)\ntrainRijnummers <- createDataPartition(y = marketing$sales, p=0.8, list = FALSE)\ntrainData <- marketing[trainRijnummers, ]\ntestData <- marketing[-trainRijnummers, ]\n\nVoor de trainData met 162 waarnemingen wordt vervolgens een lineair regressiemodel gemaakt."
  },
  {
    "objectID": "linmodel.html#regressiemodel",
    "href": "linmodel.html#regressiemodel",
    "title": "1  Lineair model",
    "section": "1.3 Regressiemodel",
    "text": "1.3 Regressiemodel\nVoor het bepalen van een lineair regressiemodel voor trainData wordt de R-functie lm() gebruikt. Via de kleinste kwadraten methode worden de coëfficienten van de regressievergelijking bepaald.\nDaarna wordt een t-test uitgevoerd om te controleren of deze coëfficiënten significant verschillen van nul. Wanneer een coëfficiënt niet nul is, dan is er een significant verband tussen de voorspelvariabele(n) \\(x\\) en de uitkomstvariabele \\(y\\).\nSyntax: lm(formula, data, ...) met\n\nformula - Dit is de modelformule, met de uitkomstvariabele aan de linkerkant van de tilde ~ en de voorspelvariabelen aan de rechterkant.\ndata - Dataframe met de variabelen in het model.\n\n\n\n\n\n\n\nOpmerking\n\n\n\nDe syntax aan de rechterkant in de modelformule is volgens de Wilkinson-Rogers specificatie. Hierbij worden de volgende symbolen gebruikt.\n\n+ om variabelen te combineren, bijvoorbeeld var1 + var2.\n: voor interacties tussen de voorspelvariabelen, bijvoorbeeld var1:var2.\n* voor zowel variabelen zelf als interacties daartussen, zo is bijvoorbeeld var1*var2 hetzelfde als var1 + var2 + var1:var2.\n. indicatie voor alle voorspelvariabelen.\n\nIn de modelformule mogen ook wiskundige functies voorkomen zoals bijvoorbeeld log() of poly().\n\n\n\nmodel <- lm(sales ~ ., data = trainData)\nmodel\n\n\nCall:\nlm(formula = sales ~ ., data = trainData)\n\nCoefficients:\n(Intercept)      youtube     facebook    newspaper  \n    3.59414      0.04464      0.18882      0.00284  \n\n\nDe regressievergelijking wordt:\n\\(sales = 3.594 + 0.0446*youtube + 0.1888*facebook + 0.00284*newspaper\\)"
  },
  {
    "objectID": "linmodel.html#voorspellingen",
    "href": "linmodel.html#voorspellingen",
    "title": "1  Lineair model",
    "section": "1.4 Voorspellingen",
    "text": "1.4 Voorspellingen\nVoorspellingen kunnen gemaakt worden met de R-functie predict. Wanneer het object een lineair model is dan wordt op de achtergrond de functie predict.lm gebruikt.\nSyntax: predict(object, newdata, ...)\n\nobject: een “lm” object\nnewdata: een dataframe met de waarden voor de voorspelvariabelen waarvoor de waarde van de uitkomstvariabele geschat moet worden.\n\nAls voorbeeld de voorspelling van sales wanneer 1000 in alledrie de media geïnvesteerd wordt.\n\nscenario1 <- data.frame(youtube = 1000, facebook = 1000, newspaper = 1000)\npredict(object = model, newdata = scenario1)\n\n  1 \n240 \n\n\nJe kunt ook meerdere voorspellingen tegelijk maken:\n\nscenario2 <- data.frame(youtube = c(1000, 2000), \n                      facebook = c(1000, 1500), \n                      newspaper = c(1000, 500))\npredict(object = model, newdata = scenario2)\n\n  1   2 \n240 378"
  },
  {
    "objectID": "linmodel.html#beoordeling-model",
    "href": "linmodel.html#beoordeling-model",
    "title": "1  Lineair model",
    "section": "1.5 Beoordeling model",
    "text": "1.5 Beoordeling model\nMet summary() krijg je meer details over het model:\n\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ ., data = trainData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.714  -0.994   0.368   1.449   3.362 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.59414    0.42081    8.54  1.1e-14 ***\nyoutube      0.04464    0.00155   28.76  < 2e-16 ***\nfacebook     0.18882    0.00953   19.82  < 2e-16 ***\nnewspaper    0.00284    0.00644    0.44     0.66    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.04 on 158 degrees of freedom\nMultiple R-squared:  0.895, Adjusted R-squared:  0.893 \nF-statistic:  451 on 3 and 158 DF,  p-value: <2e-16\n\n\n\nResiduals : Geeft een overzicht van de verdeling van de residuen. Het gemiddelde hiervan moet nul zijn wat je kunt controleren met mean(model$residuals): -2.646^{-18}.\nCoefficients : De waarden van de coëfficienten in de regressievergelijking met bijbehorende statistieken.\n\nEstimate : Geeft de waarde van de coëfficient.\nStd. Error : Standaardfout van de coëfficient. Geeft nauwkeurigheid weer: hoe lager hoe beter\nt value : Geeft de waarde van de coëfficient aan in termen van de standaardfout. t-waarde = Estimate / Std.Error\nPr(>|t|) : p-waarde voor de t-toets, die de significantie van de teststatistiek aangeeft. Hoe kleiner des te significanter (met sterren).\n\n\nDe algehele kwaliteit van de lineaire regressiefit kan worden beoordeeld aan de hand van de volgende drie grootheden, weergegeven in het modeloverzicht:\n\nResidual Standard Error (RSE)\nR-squared (\\(R^2\\)) en adjusted R-squared\nF-statistic\n\n\n1.5.1 Residual Standard Error (RSE)\nDe RSE, die overeenkomt met de voorspellingsfout, vertegenwoordigt ruwweg het gemiddelde verschil tussen de waargenomen uitkomstwaarden en de voorspelde waarden door het model. De RSE wordt uitgedrukt in dezelfde eenheid als de uitkomstvariabele. Hoe lager de RSE, hoe beter het model bij onze gegevens past.\n\\(RSE = \\frac{\\sqrt{(\\text{som residuen})^2}}{\\text{aantal vrijheidsgraden}}\\)\nAls je de RSE deelt door de gemiddelde waarde van de uitkomstvariabele, krijg je het voorspellingsfoutpercentage, dat zo klein mogelijk moet zijn.\n\n\n\n\n\n\nOpmerking\n\n\n\nIn het voorbeeld is de RSE = 2,04, wat betekent dat de waargenomen waarden van sales gemiddeld ongeveer 2,04 eenheden afwijken van de voorspelde waarden.\nDit komt overeen met een foutpercentage van 2,04/mean(trainData$sales) = 2,04/16,9 = 12%, wat laag is.\n\n\n\n\n1.5.2 R-squared en adjusted R-squared\n\\(R^2\\), weergegeven als R squared in de uitvoer, geeft aan welk deel van de totale variantie verklaard wordt door het model.\n\\(R^2 = \\frac{\\text{Verklaarde variantie}} {\\text{Totale variantie}}\\)\nDe \\(R^2\\) heeft een bereik van 0 tot 1. Voor modellen die goed bij de gegevens passen is de waarde nagenoeg 1. Voor modellen die slecht passen ligt de waarde dicht bij 0.\nEen probleem met de \\(R^2\\) is dat deze toeneemt wanneer je meer onafhankelijke variabelen aan het model toevoegt, zelfs als de nieuw toegevoegde variabelen niet aan een verbetering van het model bijdragen. Een oplossing is om de \\(R^2\\) aan te passen door rekening te houden met het aantal voorspellende variabelen. Dit doet de Adjusted R Squared, welke een correctie is voor het aantal voorspelvariabelen dat in het model is opgenomen.\nOm deze reden is het beter om de Adjusted R Squared als maatstaf te gebruiken, want deze neemt alleen toe wanneer de fout afneemt.\nAls er een aanzienlijk verschil is tussen R squared en Adjusted R Squared, geeft dit aan dat je kunt overwegen om het aantal kenmerken in het model te verkleinen.\n\n\n\n\n\n\nOpmerking\n\n\n\nVoor het voorbeeld is de Adjusted R Square 0,893 wat een goede waarde is. Dus zo’n 89% van de variabiliteit in de uitkomsten wordt verklaard door het regressiemodel.\n\n\n\n\n1.5.3 F-statistiek\nDe F-statistic geeft de overall significantie van het model weer. Het is de verhouding tussen de verklaarde en onverklaarde variantie. Het beoordeelt of ten minste één voorspellende variabele een coëfficiënt heeft die niet nul is. Wanneer de p-waarde van F-statistic klein is dan houdt dat in minstens een van de voorspelvariabelen significant gerelateerd is aan de uitkomstvariabele.\nIn een eenvoudige lineaire regressie is deze test niet echt interessant, omdat deze statistiek gewoon de informatie dupliceert die wordt gegeven door de t-test, welke beschikbaar is in de coëfficiëntentabel.\nDe F-statistiek wordt belangrijker zodra er meerdere voorspellers zijn zoals bij meervoudige lineaire regressie.\nEen grote F-statistiek komt overeen met een statistisch significante p-waarde (p < 0,05).\n\n\n\n\n\n\nOpmerking\n\n\n\nIn het voorbeeld is de F-statistic gelijk aan 451 en produceert een p-waarde <2e-16, wat zeer significant is.\n\n\n\n\n1.5.4 Significantie coëfficienten\nOnder de tekst Coefficients zie je de rijen voor Intercept en de variabelen. Aan het eind van deze rijen kunnen sterren staan. Deze sterren geven aan dat de betreffende rij significant is, dus dat het voor deze regressie van belang is dat deze variabele in het model wordt gebruikt. Des te meer sterren, des te meer significantie. Een aanduiding met één ster is al genoeg om aan te tonen dat de variabele significant is. Als er geen sterren staan is de variabele niet significant, dus niet verklarend genoeg in de regressie.\n\n\n\n\n\n\nOpmerking\n\n\n\nHet is te zien dat veranderingen in het advertentiebudget van youtube en facebook significant geassocieerd zijn met veranderingen in sales, terwijl veranderingen in newspaper niet significant geassocieerd zijn met sales."
  },
  {
    "objectID": "linmodel.html#aanpassing-model",
    "href": "linmodel.html#aanpassing-model",
    "title": "1  Lineair model",
    "section": "1.6 Aanpassing model",
    "text": "1.6 Aanpassing model\nOmdat de variabele newspaper niet significant is, kun je deze uit het model verwijderen en onderzoeken of het nieuwe model beter is.\n\nmodel2 <- lm(sales ~ youtube + facebook, data = trainData)\ncoef(model2) # coëfficienten regressievergelijking\n\n(Intercept)     youtube    facebook \n     3.6586      0.0446      0.1902 \n\n\nDe regressievergelijking wordt nu:\n\\(sales = 3.6586 + 0.0446*youtube + 0.1902*facebook\\)\nMet fitted() krijg je de berekende waarden volgens de regressievergelijking.\n\nhead(fitted(model2), 10)\n\n    1     4     5     7     8     9    10    11    12    13 \n24.61 21.20 15.81 14.22 14.57  4.60 14.96  8.52 20.64 12.94 \n\n\n\nsummary(model2)\n\n\nCall:\nlm(formula = sales ~ youtube + facebook, data = trainData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.813  -1.007   0.324   1.464   3.345 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.65858    0.39361    9.29   <2e-16 ***\nyoutube      0.04465    0.00155   28.85   <2e-16 ***\nfacebook     0.19017    0.00901   21.11   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.04 on 159 degrees of freedom\nMultiple R-squared:  0.895, Adjusted R-squared:  0.894 \nF-statistic:  680 on 2 and 159 DF,  p-value: <2e-16\n\n\nAlle coëfficienten zijn significant en de F-statistic is flink omhoog gegaan. De \\(R^2\\), de gecorrigeerde \\(R^2\\) en de RSE zijn nauwelijks veranderd.\n\n1.6.1 Anova\nMet de functie anova() krijg je een hiërarchische variantie-analyse van elk van de termen in het model, in dezelfde volgorde als in de modelformule.\n\nanova(model2)\n\nAnalysis of Variance Table\n\nResponse: sales\n           Df Sum Sq Mean Sq F value Pr(>F)    \nyoutube     1   3799    3799     915 <2e-16 ***\nfacebook    1   1852    1852     446 <2e-16 ***\nResiduals 159    660       4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n1.6.2 Residuen\nEen lineair model met goede \\(p\\)- en \\(R^2\\)-waarden hoeft niet persé een goed model te zijn. Deze waarden vertellen niet het hele verhaal. Je moet ook altijd naar de residuen (Residuals) kijken, dus de verschillen tussen de werkelijke waarden en de voorspelde waarden volgens de regressievergelijking. Residuen kunnen onverklaarde patronen onthullen in de gegevens van het toegepaste model.\nBij een goed model moet het gemiddelde en som van de residuen bij benadering zo dicht mogelijk bij nul liggen. Daarnaast moeten de waarden willekeurig verdeeld zijn. Je kunt dit beoordelen door een grafiek van de residuen te maken. Met deze informatie kun je niet alleen controleren of aan de aannames voor lineaire regressie voldaan is, maar kun je het model ook op een verkennende manier verbeteren.\nMet residuals() krijg je de individuele residuen.\n\nhead(residuals(model2))\n\n      1       4       5       7       8       9 \n 1.9068  0.9995 -0.3304 -0.0643  1.2684  1.1614 \n\n# gemiddelde van de residuen\nmean(residuals(model2))\n\n[1] -9.61e-17\n\n\n\n\n1.6.3 Diagnostische grafieken\nVoor verdere uitleg hierover zie Hoofdstuk 2.\n\nplot(model2)\n\n\n\n\n\n\n\n(a) Residuals - Fitted values\n\n\n\n\n\n\n\n(b) Normal Q-Q\n\n\n\n\n\n\n\n\n\n(c) Scale-Location\n\n\n\n\n\n\n\n(d) Residuals - Leverage\n\n\n\n\nFiguur 1.2: Diagnostische grafieken"
  },
  {
    "objectID": "linmodel.html#testen-met-testdata",
    "href": "linmodel.html#testen-met-testdata",
    "title": "1  Lineair model",
    "section": "1.7 Testen met testData",
    "text": "1.7 Testen met testData\nVoor het testen wordt het laatste, aangepaste model2 gebruikt.\n\nvoorspellingen <- predict(model2, testData)\n\nDe functie postResample(pred, obs) uit package caret kan gebruikt worden om de Root Mean Squared Error (RMSE), de eenvoudige \\(R^2\\) en de Mean Absolute Error (MAE) te bepalen. Je kunt desgewenst ook de afzonderlijke functies RMSE(pred, obs), R2(pred, obs) en MAE(pred, obs) gebruiken.\n\npostResample(pred = voorspellingen, obs = testData$sales)\n\n    RMSE Rsquared      MAE \n   1.949    0.907    1.422 \n\n\n\nRMSE\nDe RMSE is een metriek die je vertelt hoe ver de voorspelde waarden (pred = predicted) gemiddeld verwijderd zijn van de waargenomen waarden (obs = observed) in een regressieanalyse, ofwel de voorspellingsfout van het model. Hoe lager de RMSE, des te beter het model.\nHet wordt berekend via:\n\\(RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(pred_i - obs_i)^2}{n}}\\)\nDe RMSE kan gezien worden als de standaardafwijking van de onverklaarbare variantie en wordt uitgedrukt in dezelfde eenheid als de vorspelvariabele. RMSE is een goede maat voor de nauwkeurigheid waarmee het model voorspellingen doet en is een belangrijk criterium wanneer het hoofddoel het doen van voorspellingen is.\n\n\nR2\nDe R2 berekening in caret is een rechttoe rechtaan berekening van de correlatie tussen de waargenomen en voorspelde waarden (dus \\(R\\)) en daarna het kwadrateren van deze waarde. Wanneer het model slecht is, kan dit leiden tot verschillen tussen deze schatter en de meer algemeen bekende schatting afgeleid van lineaire regressiemodellen.\n\n# correlatiecoefficient R\ncor(voorspellingen, testData$sales)\n\n[1] 0.952\n\n\n\n\nMAE\nNet als de RMSE meet MAE de voorspelfout en is wat minder gevoelig voor uitbijters. De MAE wordt berekend via\n\\(MAE = \\frac{\\sum_{i=1}^{n}|pred_i - obs_i|}{n}\\)\n\n\n\n\n\n\nOpmerking\n\n\n\nDe RMSE is 1,95, wat overeenkomt met een foutpercentage van 1,95/mean(testData$sales) = 1,97/16.7 = 11,7%, wat ook goed is.\nEen R-kwadraat van 0,907 betekent dat de waargenomen en voorspelde uitkomstwaarden sterk gecorreleerd zijn, wat erg goed is.\nEn een MAE van 1,4 is ook goed."
  },
  {
    "objectID": "linmodel.html#interactie-effecten",
    "href": "linmodel.html#interactie-effecten",
    "title": "1  Lineair model",
    "section": "1.8 Interactie-effecten",
    "text": "1.8 Interactie-effecten\nHet tot nu toe gebruikte model is een additief model:\n\\(sales = b_0 + b_1*youtube + b_2*facebook\\)\nHierbij wordt verondersteld dat er geen relatie is tussen de voorspelvariabelen youtube en facebook. Deze veronderstelling is misschien niet waar. Geld uitgeven aan Facebook-advertenties kan bijvoorbeeld de effectiviteit van YouTube-advertenties op de verkoop vergroten. In marketing staat dit bekend als een synergie-effect en in statistieken wordt het een interactie-effect genoemd\nEen regressievergelijking met interactie-effecten tussen deze voorspelvariabelen kan er als volgt uitzien:\n\\(sales = b_0 + b_1*youtube + b_2*facebook + b_3*(youtube*facebook)\\)\n\n\n\n\n\n\nOpmerking\n\n\n\n\\(b_3\\) kan worden geïnterpreteerd als de toename van de effectiviteit van YouTube-advertenties voor een toename van één eenheid in Facebook-advertenties (of omgekeerd).\n\n\nHet model wordt dan\n\nmodel3 <- lm(sales ~ youtube + facebook + youtube:facebook, data=trainData)\nsummary(model3)\n\n\nCall:\nlm(formula = sales ~ youtube + facebook + youtube:facebook, data = trainData)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.632 -0.505  0.267  0.743  1.811 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      8.18e+00   3.31e-01   24.74   <2e-16 ***\nyoutube          1.86e-02   1.66e-03   11.20   <2e-16 ***\nfacebook         2.78e-02   1.02e-02    2.74   0.0069 ** \nyoutube:facebook 9.14e-04   4.95e-05   18.47   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.15 on 158 degrees of freedom\nMultiple R-squared:  0.967, Adjusted R-squared:  0.966 \nF-statistic: 1.54e+03 on 3 and 158 DF,  p-value: <2e-16\n\nvoorspellingen <- predict(object = model3, newdata = testData)\n\nRMSE(voorspellingen, testData$sales)\n\n[1] 1.06\n\nR2(voorspellingen, testData$sales)\n\n[1] 0.972\n\n\nInterpretatie\nHet is te zien dat alle coëfficiënten, inclusief de interactietermcoëfficiënt, statistisch significant zijn, wat suggereert dat er een interactierelatie is tussen de twee voorspellende variabelen youtube en facebook.\nDe regressievergelijking ziet er nu als volgt uit:\n\\(sales = 8.18 + 0,0186*youtube + 0,0278*facebook + 0,0009*youtube*facebook\\)\nDe RMSE van het interactiemodel is 1,06 wat lager is dan de 1,95 van het additieve model.Aanvullend is de \\(R^2\\) van het interactiemodel 97% en hoger dan de 91% van het additieve model.\nDeze resultaten suggereren dat het model met de interactieterm beter is dan het model dat alleen hoofdeffecten bevat. Voor deze specifieke data is dus het interactiemodel het aangewezen model."
  },
  {
    "objectID": "linmodel.html#kruisvalidatie",
    "href": "linmodel.html#kruisvalidatie",
    "title": "1  Lineair model",
    "section": "1.9 Kruisvalidatie",
    "text": "1.9 Kruisvalidatie\nNa het bouwen van een model wil je weten hoe nauwkeurig het model is bij het voorspellen van de uitkomst voor nieuwe waarnemingen die niet zijn gebruikt om het model te bouwen. Met andere woorden, je wilt de voorspellingsfout schatten. Een van de manieren voor het valideren van het model op de testgegevens is kruisvalidatie (cross-validation).\nKruisvalidatie staat ook bekend als een resampling-methode (herbemonstering) omdat het inhoudt dat dezelfde statistische methode meerdere keren wordt aangepast met behulp van verschillende subsets van de gegevens. De meest gebruikte kruisvalidatiemethodes zijn de k-voudige kruisvalidatie en de herhaalde k-voudige kruisvalidatie.\nDe statistieken die gebruikt zullen worden voor een beoordeling van de kwaliteit van een model zijn\n\nR-squared (\\(R^2\\)) : hoe hoger, hoe beter\nRMSE (gemiddelde voorspellingsfout) : hoe lager, hoe beter\nMEA (gemiddelde absolute fout) : hoe lager, hoe beter\n\n\n1.9.1 k-voudige kruisvalidatie\nDeze methode evalueert de modelprestaties op verschillende subsets van de trainingsgegevens en berekent vervolgens de gemiddelde voorspellingsfoutpercentage. Het algoritme is als volgt:\n\nSplits de dataset willekeurig in k-subsets (of k-voudig), bijvoorbeeld 5 subsets.\nReserveer één subset voor het testen en train het model op alle andere subsets.\nTest het model op de gereserveerde subset en bepaal de voorspellingsfout\nHerhaal dit proces totdat elk van de k subsets als testset heeft gediend.\nBereken het gemiddelde van de k geregistreerde fouten. Dit wordt de kruisvalidatiefout genoemd en dient als prestatiestatistiek voor het model.\n\nTypische vraag, is hoe de juiste waarde van k te kiezen?\nIn de praktijk voert men doorgaans een k-voudige kruisvalidatie uit met k = 5 of k = 10, aangezien empirisch is aangetoond dat deze waarden schattingen van het testfoutpercentage opleveren die niet lijden onder een extreem hoge bias of een zeer hoge variantie.\nMet de functie trainControl worden de details voor de resampling gespecificeerd.\nSyntax: trainControl( method, number, repeats, ...)\n\nmethod: De resampling methode, bijv. “boot”, “cv”, “repeatedcv”, …\nnumber: het aantal vouwen (subsets) of resampling iteraties\nrepeats: het aantal keren dat herhaald moet worden, alleen voor methode “repeatedcv”\n\nToegepast op het model met de interactie\n\n# Data\ndata(\"marketing\", package = \"datarium\")\n\n# Define training control\nset.seed(123) \ntrain.control <- trainControl(method = \"cv\", number = 10)\n# Train the model\nmodel4 <- train(sales ~ youtube + facebook + youtube:facebook, data = marketing, method = \"lm\",\n               trControl = train.control)\n# Summarize the results\nprint(model4)\n\nLinear Regression \n\n200 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 179, 180, 180, 180, 182, 180, ... \nResampling results:\n\n  RMSE  Rsquared  MAE  \n  1.07  0.974     0.812\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\nmodel4$finalModel\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nCoefficients:\n       (Intercept)             youtube            facebook  `youtube:facebook`  \n          8.100264            0.019101            0.028860            0.000905  \n\n\n\n\n1.9.2 Herhaalde k-voudige kruisvalidatie\nBij deze methode wordt het proces van het splitsen van de data in k subsets een aantal keren herhaald. De uiteindelijke modelfout wordt dan de gemiddelde fout van het aantal herhalingen.\n\n# Define training control\nset.seed(123)\ntrain.control <- trainControl(method = \"repeatedcv\", \n                              number = 10, repeats = 3)\n# Train the model\nmodel5 <- train(sales ~ youtube + facebook + youtube:facebook, data = marketing, method = \"lm\",\n               trControl = train.control)\n# Summarize the results\nprint(model5)\n\nLinear Regression \n\n200 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 179, 180, 180, 180, 182, 180, ... \nResampling results:\n\n  RMSE  Rsquared  MAE  \n  1.08  0.971     0.809\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\nmodel5$finalModel\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nCoefficients:\n       (Intercept)             youtube            facebook  `youtube:facebook`  \n          8.100264            0.019101            0.028860            0.000905  \n\n\n\n\n\n\n1. ‘Fisher’s Exact Test’, Wikipedia, (2022).\n\n\n2. ‘Unusual Observations - Outlier, Leverage, and Influential Points’, https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A., ‘Statistiek om mee te werken’, Third dr., Stenfert Kroese, (1991).\n\n\n4. Chouldechova, A., ‘Regression Diagnostic Plots’, https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, ‘R Linear Regression Tutorial: Lm Function in R with Code Examples’, https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., ‘Finding a Suitable Linear Model for Ozone Prediction’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M., ‘Interpreting Linear Prediction Models’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, ‘Beginners Guide to Regression Analysis and Plot Interpretations’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, ‘Practical Guide to Logistic Regression Analysis in R’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A., ‘Logistic Regression Essentials in R’, 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., ‘Machine Learning Essentials: Practical Guide in R’, 1st edition dr., (2018).\n\n\n12. University of Virginia, ‘Understanding Diagnostic Plots for Linear Regression Analysis | University of Virginia Library Research Data Services + Sciences’, https://data.library.virginia.edu/diagnostic-plots/."
  },
  {
    "objectID": "diagnoses.html#ideaal-model",
    "href": "diagnoses.html#ideaal-model",
    "title": "2  Diagnostische grafieken",
    "section": "2.1 Ideaal model",
    "text": "2.1 Ideaal model\nBron: Regression diagnostic plots, Chouldechova\nVoor het maken van vergelijkingen met een ideaal model wordt een gegevensverzameling gemaakt welke perfect voldoet aan alle standaardeisen voor lineaire regressie. De data wordt gegenereerd voor de lineaire vergelijking\n\\[y_i = 3 + 0.1 x + \\epsilon_i,\\]\nmet \\(i = 1, 2, \\ldots, 1000\\), met \\(\\epsilon_i \\sim N(0, 3)\\).\nDe x-waarden worden gegenereerd uit een uniforme verdeling [0, 100]\n\nn <- 500      # aantal waarnemingen\nset.seed((12345))\nx <- runif(n, min = 0, max = 100)\ny.ideaal <- 3 + 0.1 * x + rnorm(n, sd = 3)\n\n# ideale lineaire model\nlm.ideaal <- lm(y.ideaal ~ x)\n\nplot(x, y.ideaal, main = \"ideaal lineair model\")\nabline(lm.ideaal, col = \"blue\") # regressielijn toevoegen\n\n\n\n\nFiguur 2.1: Spreidingsdiagram voor een ideaal lineair model met regressielijn.\n\n\n\n\n\n\n\nHet spreidingsdiagram toont een perfecte lineaire regressie: de punten lijken willekeurig verspreid over de lijn, zonder waarneembare niet-lineaire trends of niet-constante variantie.\n\n2.1.1 Residual vs. Fitted\nIn deze grafiek worden de residuen uitgezet tegen de voorspelde waarden. Idealiter mag deze grafiek geen patroon vertonen. Wanneer je wel een patroon (curve, U-vorm) ziet dan suggereert dit niet-lineariteit in de dataset. Als je gelijk verdeelde residuen rond een horizontale lijn vindt zonder duidelijke patronen, is dat een goede indicatie dat je lineaire relaties hebt. Als je bovendien een trechtervormig patroon ziet, dan suggereert dit heteroskedasticiteit, d.w.z. dat de fouttermen een niet-constante variantie hebben.\n\nplot(lm.ideaal, which = 1)\n\n\n\n\nFiguur 2.2: Residuals-fitted voor het ideale model.\n\n\n\n\nDe rode lijn kun je zien als een soort regressielijn voor de residuen en deze toont de gemiddelde waarde van de residuen voor elke berekende waarde (fitted value). Deze rode lijn is vlak en wijst er op dat er geen waarneembare niet-lineaire trend in de residuen is. Verder lijken de residuen even variabel te zijn over het gehele bereik van passende waarden. Er is geen indicatie van niet-constante variantie.\n\n\n2.1.2 Normal Q-Q\nMet deze grafiek kun je zien of residuen ruwweg normaal verdeeld zijn. Het maakt gebruik van gestandaardiseerde waarden van residuen. Idealiter zou deze grafiek een rechte lijn moeten vertonen. Vind je een gekromde, vervormde lijn, dan hebben je residuen een niet-normale verdeling (problematische situatie).\nWanneer de residuen de neiging hebben om groter in omvang te zijn dan wat je zou verwachten van de normale verdeling, dan kunnen de p-waarden en betrouwbaarheidsintervallen te optimistisch zijn.\n\nplot(lm.ideaal, which = 2)\n\n\n\n\nFiguur 2.3: Normal QQ grafiek voor het ideale model.\n\n\n\n\nDe residuen vormen hier een goede match met de diagonale lijn. De residuen lijken dus normaal verdeeld te zijn.\n\n\n2.1.3 Scale-Location\nMet deze grafiek (ook wel Spread-Location genoemd) kun je zien of residuen gelijkelijk zijn verdeeld over de reeksen van voorspellers. Zo kun je de aanname van gelijke variantie (homoscedasticiteit) controleren. Het is goed als je een horizontale lijn ziet met gelijke (willekeurige) spreidingspunten.\nDeze grafiek is een meer gevoelige benadering voor het zoeken naar afwijkingen van de aanname van constante variantie. Als je significante trends in de rode lijn op deze grafiek ziet, dan is dat een aanwijzing dat de residuen (en dus fouten) een niet-constante variantie hebben. Dat wil zeggen, de aanname dat alle \\(\\epsilon_i\\) dezelfde variantie \\(\\sigma^2\\) hebben, is niet waar.\n\nplot(lm.ideaal, which = 3)\n\n\n\n\nFiguur 2.4: Scale-Location grafiek voor het ideale model.\n\n\n\n\nEen horizontale rode lijn is ideaal en geeft aan dat de residuen een uniforme variatie hebben binnen het bereik van de verklarende variabele. Naarmate de residuen zich verder van elkaar verspreiden, gaat de rode lijn omhoog.\nDe lijn is hier behoorlijk plat, wat betekent dit de fouten een constante variantie hebben, zoals gewenst.\n\n\n2.1.4 Cook’s Distance\nCook’s Distance, vaak aangeduid met \\(D_i\\), wordt gebruikt om invloedrijke gegevenspunten te identificeren die een negatief effect kunnen hebben op uw regressiemodel. Het feit dat een gegevenspunt invloedrijk is, betekent niet dat het noodzakelijkerwijs moet worden verwijderd. Controleer eerst of het gegevenspunt gewoon onjuist is vastgelegd of dat er iets vreemds aan het gegevenspunt is dat op een interessante bevinding kan wijzen.\nIn wezen doet Cook’s Distance één ding: het meet hoeveel alle berekende waarden in het model veranderen wanneer gegevenspunt \\(i\\) wordt verwijderd.\nEen gegevenspunt met een grote waarde voor Cook’s Distance geeft aan dat het de berekende waarden sterk beïnvloedt. Een algemene vuistregel is dat elk punt met een Cook’s Distance groter dan 4/n (waarbij n het totale aantal gegevenspunten is) als een uitbijter wordt beschouwd.\nMet deze grafiek worden de Cook’s afstanden voor elke waarneming bepaald en in een grafiek uitgezet.\n\nplot(lm.ideaal, which = 4)\n\n\n\n\nFiguur 2.5: Cook’s afstanden voor elke waarneming van het ideale model.\n\n\n\n\n\n\n2.1.5 Residuals vs Leverage\nLeverage is een maatstaf voor de mate waarin een waarneming het passen (fit) van het model beinvloedt. Het is een ééncijferige samenvatting van hoe verschillend de fit van het model zou zijn als de betreffende waarneming niet mee zou tellen.\nPunten met een hoog residu (=slecht beschreven door het model) en een hoge leverage (= grote invloed op de modelfit) zijn uitschieters. Ze trekken het model weg van de rest van de gegevens en lijken niet echt te passen bij de rest van de gegevens.\nDeze grafiek helpt je om invloedrijke waarnemingen te vinden als ze er zijn. Niet alle uitschieters zijn invloedrijk in de lineaire regressie-analyse. Zelfs als de gegevens extreme waarden hebben, zijn ze mogelijk niet van invloed op het bepalen van een regressielijn. Dat betekent dat de resultaten niet veel anders zouden zijn als ze in- of uitgesloten zouden worden bij de analyse. Ze volgen in de meeste gevallen de trend en ze doen er niet echt toe; ze zijn niet invloedrijk. Aan de andere kant kunnen sommige gevallen zeer invloedrijk zijn, zelfs als ze binnen een redelijk bereik van de waarden lijken te liggen. Dit kunnen extreme gevallen zijn voor een regressielijn en kunnen de resultaten wijzigen wanneer ze zouden worden uitgesloten bij de analyse. Een andere manier om het te zeggen is dat ze in de meeste gevallen niet aansluiten bij de trend.\n\nplot(lm.ideaal, which = 5)\n\n\n\n\nFiguur 2.6: Residuals-Leverage grafiek voor het ideale model.\n\n\n\n\nDeze grafiek is een goed voorbeeld dat er geen bewijs voor uitschieters is. De gestippelde Cook’s afstandslijnen zijn zelfs niet aanwezig in de grafiek. Geen van de punten komt in de buurt van zowel een groot residu als een grote leverage.\n\n\n2.1.6 Cook’s dist vs Leverage\nIn deze grafiek zijn contouren van gestandaardiseerde residuen die even groot zijn, lijnen door de oorsprong. De contourlijnen zijn gelabeld met de magnitudes.\nDe afstand van Cook is groot als de waarneming een groot residu heeft of als deze een grote invloed op het model uitoefent.\n\nplot(lm.ideaal, which = 6)\n\n\n\n\nFiguur 2.7: Cook’s distance-Leverage grafiek voor het ideale model.\n\n\n\n\n\n\n2.1.7 Samenvatting\nDe diagnostische grafieken geven informatie over het model en de gegevens. Het kan zijn dat het gebruikte model niet de beste manier is om de gegevens te begrijpen en dat er waardevolle informatie in de data is achtergebleven. In dat geval wil je misschien terug naar je theorie en hypothesis. Is er werkelijk een lineaire relatie tussen de onafhankelijke variabele(n) en de afhankelijke variabele? Misschien moet er bijvoorbeeld een kwadratische term worden opgenomen. Of geeft een logaritmische transformatie het verschijnsel dat je wilt modelleren beter weer. Of er is misschien een belangrijke variabele die niet in het model is opgenomen? Of misschien waren de gegevens systematisch vertekend (systematische ruis) bij het verzamelen hiervan, waardoor je daar dan een nieuwe methode voor moet ontwerpen.\nVoor verdere verdieping zie het artikel Unusual Observations - Outlier, Leverage, and Influential Points"
  },
  {
    "objectID": "diagnoses.html#niet-ideale-modellen",
    "href": "diagnoses.html#niet-ideale-modellen",
    "title": "2  Diagnostische grafieken",
    "section": "2.2 Niet-ideale modellen",
    "text": "2.2 Niet-ideale modellen\n\n2.2.1 Gebogen trend\nHier is een voorbeeld waarbij niet-lineaire trends in de gegevens aanwezig zijn. Dit voorbeeld is gemaakt om seizoensgegevens na te bootsen.\n\ny.gebogen <- 5 * sin(0.6 * x) + 0.1 * x + rnorm(n, sd = 2)\n\n# Scatterplot van de data\nggplot(mapping = aes(x, y.gebogen)) +\n    geom_point() + \n    stat_smooth(method = \"lm\") + \n    stat_smooth(method = \"loess\", span = 0.1, colour = I(\"red\"))\n\n\n\n\nFiguur 2.8: Gegevens met een seizoentrend\n\n\n\n\nDe blauwe lijn geeft het lineaire model weer. De rode curve is een niet-lineaire aanpassing die de gemiddelde waarde van y beter modelleert bij elke waarde van x. Merk op dat het lineaire model er niet in slaagt de duidelijke niet-lineaire trend vast te leggen die aanwezig is in de gegevens. Dit veroorzaakt enorme problemen voor onze gevolgtrekking. Kijk naar de grijze betrouwbaarheidsband die de regressielijn omringt. Als aan de standaardaannames voor lineaire regressie wordt voldaan, zou deze band met hoge waarschijnlijkheid de gemiddelde waarde van y bevatten bij elke waarde van x. d.w.z. de grijze banden rond de blauwe curve zouden grotendeels de rode curve moeten bevatten. Dit gebeurt uiteraard niet. De rode curve ligt bijna altijd ver buiten de grijze banden rond de blauwe regressielijn.\n\nlm.gebogen <- lm(y.gebogen ~ x)\nplot(lm.gebogen, which = 1)\n\n\n\n\nFiguur 2.9: Residuals-fitted voor het gekromde model.\n\n\n\n\nVisueel is een duidelijke trend in de residuen te zien. Ze hebben een periodieke trend. Helaas doet de scatterplot smoother die wordt gebruikt om de rode lijn te construeren hier geen goed werk. Dit is een geval waarin de keuze van buurtgrootte (hoeveel punten er nodig zijn om het lokale gemiddelde te berekenen) te groot wordt geacht om de trend die we visueel waarnemen vast te leggen. Vertrouw die rode curve niet altijd.\n\nplot(lm.gebogen, which = 2)\n\n\n\n\nFiguur 2.10: Normal Q-Q voor het gekromde model.\n\n\n\n\nIn de QQ-plot is te zien dat de residuen afwijken van de diagonale lijn in zowel de bovenste als de onderste staart. Deze grafiek gaf aan dat de staarten ‘lichter’ zijn (kleinere waarden hebben) dan wat we zouden verwachten onder de standaardmodelleringsaannames. Dit wordt aangegeven door de punten die een “plattere” lijn vormen dan de diagonaal.\n\n\n2.2.2 Niet constante variantie\nVoor dit voorbeeld worden gegevens gegenereerd waarbij de foutvariantie toeneemt met \\(x\\). Het model wordt\n\\[\ny_i = 3 + 0.2x_i + \\epsilon_i\n\\] met \\[\\epsilon_i \\sim N(0, 9(1 + x/25)^2)\\].\n\ny.toenemend <- 3 + 0.2 * x + (1 + x / 25) * rnorm(n, sd = 3)\nplot(x, y.toenemend, ylab = \"y\")\n\n\n\n\nFiguur 2.11: Gegevens met een toenemende variantie.\n\n\n\n\n\nlm.toenemend <- lm(y.toenemend ~ x)\nplot(lm.toenemend, which = 1)\n\n\n\n\nFiguur 2.12: Residuals-fitted voor het model met toenemende variantie.\n\n\n\n\nAls je naar deze plot kijkt, zie je dat er een duidelijk “trechter”-fenomeen is. De verdeling van de residuen is vrij goed geconcentreerd rond 0 voor kleine aangepaste waarden, maar ze worden meer en meer verspreid naarmate de aangepaste waarden toenemen. Dit is een voorbeeld van “toenemende variantie”.\n\nplot(lm.toenemend, which = 2)\n\n\n\n\nFiguur 2.13: Normal Q-Q voor het model met toenemende variantie.\n\n\n\n\nIn deze Q-Q plot wijken de residuen af van de diagonale lijn in zowel de bovenste als de onderste staart. Hier zijn de staarten ‘zwaarder’ (hebben grotere waarden) dan je zou verwachten op basis van de standaardmodelleringsaannames. Dit wordt aangegeven door de punten die een “steilere” lijn vormen dan de diagonaal.\n\nplot(lm.toenemend, which = 3)\n\n\n\n\nFiguur 2.14: Scale-Location voor het model met toenemende variantie.\n\n\n\n\nLet op de duidelijke opwaartse helling in de rode trendlijn. Dit wijst op een niet-constante variantie.\nDe standaardaanname van lineaire regressie is dat de variantie constant is over het gehele bereik. Als deze aanname niet geldig is, zoals in dit voorbeeld, mag je de betrouwbaarheidsintervallen, voorspellingsbanden of de p-waarden in deze regressie niet geloven.\n\n\n2.2.3 Uitschieters\n\nset.seed(12345)\ny.aangepast <- y.ideaal[1:n]\nx.aangepast <- x[1:n]\n\n# Selecteer 10 willekeurige punten om aan te passen\nto.aangepast <- sample(1:length(x.aangepast), 10)\ny.aangepast[to.aangepast] <- - 1.5 * y.aangepast[to.aangepast] + 3 * rt(10, df = 3)\nx.aangepast[to.aangepast] <- x.aangepast[to.aangepast] * 2.5\n# Fit regression and display diagnostic plot\nlm.aangepast <- lm(y.aangepast ~ x.aangepast)\n\nplot(lm.aangepast, which = 5)\n\n\n\n\nIn deze grafiek zis te zien dat er verschillende punten zijn met een hoge restwaarde en een hoge hefboomwerking (leverage). De punten die dicht bij of buiten de gestippelde curven liggen, zijn de moeite waard om verder te onderzoeken."
  },
  {
    "objectID": "diagnoses.html#inkomen-spaargeld",
    "href": "diagnoses.html#inkomen-spaargeld",
    "title": "2  Diagnostische grafieken",
    "section": "2.3 Inkomen-Spaargeld",
    "text": "2.3 Inkomen-Spaargeld\n\nsparen <- read_csv(\"data/inkomen-spaargeld.csv\", show_col_types = FALSE)\nmodel.sparen = lm(Spaargeld ~ Inkomen , data = sparen)\n\n# Diagnostische grafiekens\nplot(model.sparen)\n\n\n\n\n\n\n\n(a) Residuals - Fitted values\n\n\n\n\n\n\n\n(b) Normal Q-Q\n\n\n\n\n\n\n\n\n\n(c) Scale-Location\n\n\n\n\n\n\n\n(d) Residuals - Leverage\n\n\n\n\nFiguur 2.15: De vier diagnostische grafieken die standaard getoond worden.\n\n\n\nJe ziet vaak nummers bij sommige punten in deze grafieken. Dat zijn extreme waarden op basis van elk criterium en geidentificeerd door de rijnummers in de gegevensverzameling. Wanneer sommige nummers in alle vier de grafieken verschijnen is er reden om de waarneming individueel te bekijken. Is er iets speciaals met dit punt aan de hand? Of is er een fout gemaakt bij het invoeren van de gegevens?\nResiduals vs Fitted\n\nplot(model.sparen, which = 1)\n\n\n\n\nFiguur 2.16: Residuals-Fitted.\n\n\n\n\nDe residuen vertonen een gebogen patroon (paraboolachtig). Dit zou kunnen betekenen dat je een beter model kunt krijgen door een kwadratische term aan het model toe te voegen.\nNormal Q-Q\n\nplot(model.sparen, which = 2)\n\n\n\n\nFiguur 2.17: Diagnostische grafiek Normal Q-Q.\n\n\n\n\nDe punten volgen de stippellijn goed, behalve waarneming 22. De residuen van het model voldoen hiermee aan de test voor een normale verdeling.\nScale-Location\n\nplot(model.sparen, which = 3)\n\n\n\n\nFiguur 2.18: Diagnostische grafiek Scale-Location.\n\n\n\n\nTot ongeveer 100000 is sprake van homoscedasticiteit (dus uniforme variatie), daarna wordt het heteroscedasticiteit.\nCook’s distance\n\nplot(model.sparen, which = 4)\n\n\n\n\nFiguur 2.19: Diagnostische grafiek Cook’s distance.\n\n\n\n\nDuidelijk is te zien dat waarneming 22 afwijkt van de rest en hierdoor mogelijk het model slecht beinvloedt. Afhankelijk van de oorzaak kun je het punt verwijderen of de waarde corrigeren. Maar het is natuurlijk ook mogelijk dat het een geldig punt is.\n\n\n\n\n\n\nOpmerking\n\n\n\nAlternatief\nplot(cooks.distance(model.sparen), pch = 16, col = \"blue\")\n\n\nResiduals vs Leverage\n\nplot(model.sparen, which = 5)\n\n\n\n\nFiguur 2.20: Diagnostische grafiek Residuals-Leverage.\n\n\n\n\nDe gestippelde lijnen zijn de Cooks afstanden. De interessegebieden zijn de gebieden buiten de stippellijn in de rechterbovenhoek of rechteronderhoek. De punten hierin kunnen een grote invloed hebben op het regressiemodel. Dergelijke punten hebben een hoge leverage of de potentie voor beinvloeding van het model is groter wanneer je die punten uitsluit.\nIn dit voorbeeld heeft waarneming 22 grote invloed op het regressiemodel.\nCook’s dist vs Leverage\n\nplot(model.sparen, which = 6)\n\n\n\n\nFiguur 2.21: Diagnostische grafiek Cook’s dist vs Leverage."
  },
  {
    "objectID": "diagnoses.html#niet-grafische-methoden",
    "href": "diagnoses.html#niet-grafische-methoden",
    "title": "2  Diagnostische grafieken",
    "section": "2.4 Niet grafische methoden",
    "text": "2.4 Niet grafische methoden\nDurbin Watson Statistiek (DW)\nDeze test wordt gebruikt om autocorrelatie te controleren. De waarde ligt tussen 0 en 4. Een DW=2 waarde vertoont geen autocorrelatie. Een waarde tussen 0 < DW < 2 impliceert echter positieve autocorrelatie, terwijl 2 < DW < 4 negatieve autocorrelatie impliceert.\n\ncar::durbinWatsonTest(lm.ideaal)\n\n lag Autocorrelation D-W Statistic p-value\n   1        -0.00932          2.02   0.902\n Alternative hypothesis: rho != 0\n\n\nVariantie-inflatiefactor (VIF)\nDeze statistiek wordt gebruikt om multicollineariteit te controleren. VIF <=4 impliceert geen multicollineariteit, maar VIF >=10 suggereert hoge multicollineariteit. Als alternatief kunt u ook naar de tolerantiewaarde (1/VIF) kijken om de correlatie in IV’s te bepalen. Daarnaast kun je ook een correlatiematrix maken om collineaire variabelen te bepalen.\nBreusch-Pagan / Cook Weisberg-Test\nDeze test wordt gebruikt om de aanwezigheid van heteroskedasticiteit te bepalen. Als je p < 0,05 vindt, verwerp je de nulhypothese en concludeer je dat heteroskedasticiteit aanwezig is.\n\n\n\n\n1. ‘Fisher’s Exact Test’, Wikipedia, (2022).\n\n\n2. ‘Unusual Observations - Outlier, Leverage, and Influential Points’, https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A., ‘Statistiek om mee te werken’, Third dr., Stenfert Kroese, (1991).\n\n\n4. Chouldechova, A., ‘Regression Diagnostic Plots’, https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, ‘R Linear Regression Tutorial: Lm Function in R with Code Examples’, https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., ‘Finding a Suitable Linear Model for Ozone Prediction’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M., ‘Interpreting Linear Prediction Models’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, ‘Beginners Guide to Regression Analysis and Plot Interpretations’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, ‘Practical Guide to Logistic Regression Analysis in R’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A., ‘Logistic Regression Essentials in R’, 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., ‘Machine Learning Essentials: Practical Guide in R’, 1st edition dr., (2018).\n\n\n12. University of Virginia, ‘Understanding Diagnostic Plots for Linear Regression Analysis | University of Virginia Library Research Data Services + Sciences’, https://data.library.virginia.edu/diagnostic-plots/."
  },
  {
    "objectID": "catregressie.html#data",
    "href": "catregressie.html#data",
    "title": "3  Categoriale Regressie",
    "section": "3.1 Data",
    "text": "3.1 Data\nDataset Salaries uit package carData bevat het academisch salaris van negen maanden voor 2008-2009 bevat voor assistent-professoren, universitair hoofddocenten en professoren aan een universiteit in de VS. De gegevens zijn verzameld om o.a. de salarisverschillen tussen mannelijke en vrouwelijke faculteitsleden te controleren.\nVariabelen\n\nrank : rang\ndiscipline\nyrs.since.phd\nyrs.service : aantal dienstjaren\nsex : geslacht\nsalary : salaris\n\n\ndata(\"Salaries\", package = \"carData\")\nhead(Salaries)\n\n       rank discipline yrs.since.phd yrs.service  sex salary\n1      Prof          B            19          18 Male 139750\n2      Prof          B            20          16 Male 173200\n3  AsstProf          B             4           3 Male  79750\n4      Prof          B            45          39 Male 115000\n5      Prof          B            40          41 Male 141500\n6 AssocProf          B             6           6 Male  97000\n\nstr(Salaries)\n\n'data.frame':   397 obs. of  6 variables:\n $ rank         : Factor w/ 3 levels \"AsstProf\",\"AssocProf\",..: 3 3 1 3 3 2 3 3 3 3 ...\n $ discipline   : Factor w/ 2 levels \"A\",\"B\": 2 2 2 2 2 2 2 2 2 2 ...\n $ yrs.since.phd: int  19 20 4 45 40 6 30 45 21 18 ...\n $ yrs.service  : int  18 16 3 39 41 6 23 45 20 18 ...\n $ sex          : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 1 ...\n $ salary       : int  139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ..."
  },
  {
    "objectID": "catregressie.html#categorievariabelen-met-2-niveaus",
    "href": "catregressie.html#categorievariabelen-met-2-niveaus",
    "title": "3  Categoriale Regressie",
    "section": "3.2 Categorievariabelen met 2 niveaus",
    "text": "3.2 Categorievariabelen met 2 niveaus\nStel dat je het verschil in salaris tussen mannen en vrouwen wilt onderzoeken. De regressievergelijking zou er dan uit kunnen zien als\n\\(salary = b_0 + b1*sex\\)\nEchter sex is een categorievariabele met de waarden Male en Female. Daar kun je niet mee rekenen. Op basis van de variabele sex kun je een nieuwe dummy variabele maken met de waarde 1 voor Male en 0 voor Female. Wanneer je deze variabele als voorspeller in de regressievergelijking gaat gebruiken dan kan dat tot het volgende model leiden:\n\n\\(salary = b_0 + b1\\) als de persoon mannelijk is\n\\(salary = b_0\\) als de persoon vrouwelijk is\n\nDe coëfficienten kun je dan als volgt interpreteren.\n\n\\(b_0\\) is het gemiddelde salaris voor vrouwen\n\\(b_0 + b_1\\) is het gemiddelde salaris voor mannen\n\\(b_1\\) is het gemiddelde verschil in salaris tussen mannen en vrouwen\n\nVoor eenvoudige demonstratiedoeleinden modelleert het volgende voorbeeld het salarisverschil tussen mannen en vrouwen door een eenvoudig lineair regressiemodel te berekenen voor de dataset Salaries. R maakt automatisch dummy-variabelen.\n\nmodel <- lm(salary ~ sex, data = Salaries)\nsummary(model)\n\n\nCall:\nlm(formula = salary ~ sex, data = Salaries)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57290 -23502  -6828  19710 116455 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   101002       4809   21.00   <2e-16 ***\nsexMale        14088       5065    2.78   0.0057 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30000 on 395 degrees of freedom\nMultiple R-squared:  0.0192,    Adjusted R-squared:  0.0167 \nF-statistic: 7.74 on 1 and 395 DF,  p-value: 0.00567\n\n\n\n101002 is het gemiddelde salaris voor vrouwen\n101002 + 14088 = 115090 is het gemiddelde salaris voor mannen\n\nDe p-waarde voor de dummyvariabele sexMale is zeer significant, wat suggereert dat er statistisch gezien een verschil in gemiddeld salaris tussen de geslachten is.\nDe functie contrasts() retourneert de codering die R heeft gebruikt om de dummy-variabelen te maken:\n\ncontrasts(Salaries$sex)\n\n       Male\nFemale    0\nMale      1\n\n\nR heeft een dummy-variabele sexMale gemaakt die de waarde 1 aanneemt als het geslacht Male is en anders 0. De beslissing om mannen als 1 te coderen en vrouwen als 0 (basline) is willekeurig en heeft geen effect op de regressieberekening, maar verandert wel de interpretatie van de coëfficiënten."
  },
  {
    "objectID": "catregressie.html#categorievariabelen-met-meer-dan-2-niveaus",
    "href": "catregressie.html#categorievariabelen-met-meer-dan-2-niveaus",
    "title": "3  Categoriale Regressie",
    "section": "3.3 Categorievariabelen met meer dan 2 niveaus",
    "text": "3.3 Categorievariabelen met meer dan 2 niveaus\nOver het algemeen wordt een categorische variabele met n niveaus omgezet in n-1 variabelen met elk twee niveaus. Deze n-1 nieuwe variabelen bevatten dezelfde informatie als de enkele variabele. Deze hercodering creëert een tabel met de naam contrastmatrix.\nZo heeft in salarie de variabele rank drie niveaus: “AsstProf”, “AssocProf” en “Prof”. Deze variabele kan dummy worden gecodeerd in twee variabelen, een genaamd AssocProf en een Prof:\n\nAls rank = AssocProf, dan zou de kolom AssocProf worden gecodeerd met een 1 en Prof met een 0.\nAls rank = Prof, dan zou de kolom AssocProf worden gecodeerd met een 0 en Prof zou worden gecodeerd met een 1.\nAls rank = AsstProf, dan worden beide kolommen AssocProf en Prof gecodeerd met een 0.\n\nDeze dummy-codering wordt automatisch uitgevoerd door R. Voor demonstratiedoeleinden kun je de functie model.matrix() gebruiken om een contrastmatrix te maken voor een factorvariabele.\n\nres <- model.matrix(~rank, data = Salaries)\nhead(res[,-1])\n\n  rankAssocProf rankProf\n1             0        1\n2             0        1\n3             0        0\n4             0        1\n5             0        1\n6             1        0\n\n\nBij het bouwen van een lineair model zijn er verschillende manieren om categorische variabelen te coderen, ook wel contrastcoderingssystemen genoemd. De standaardoptie in R is om het eerste niveau van de factor als referentie te gebruiken en de overige niveaus te interpreteren ten opzichte van dit niveau.\n\n\n\n\n\n\nOpmerking\n\n\n\nANOVA (variantieanalyse) is een speciaal geval van een lineair model waarbij de voorspellers categorische variabelen zijn. En omdat R begrijpt dat ANOVA en regressie beide voorbeelden zijn van lineaire modellen, kun je de klassieke ANOVA-tabel extraheren uit het regressiemodel met behulp van de R-basis anova() functie of de Anova()functie in package car. Over het algemeen is de functie Anova() aan te raden omdat deze automatisch voor ongebalanceerde ontwerpen zorgt.\n\n\nResultaten voor een meervoudig regressiemodel.\n\nmodel2 <- lm(salary ~ yrs.service + rank + discipline + sex, data = Salaries)\ncar::Anova(model2)\n\nAnova Table (Type II tests)\n\nResponse: salary\n              Sum Sq  Df F value  Pr(>F)    \nyrs.service 3.24e+08   1    0.63    0.43    \nrank        1.03e+11   2  100.26 < 2e-16 ***\ndiscipline  1.74e+10   1   33.86 1.2e-08 ***\nsex         7.77e+08   1    1.51    0.22    \nResiduals   2.01e+11 391                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWanneer je ook rekening houdt met andere variabelen (yrs.service, rank en discipline) zul je zien dat de categorische variabele sex niet langer significant geassocieerd is met de variatie in salary tussen individuen. Belangrijke variabelen zijn rank en discipline.\nAls je de contrasten van de categorische variabele wilt interpreteren, gebruik dan de volgende opdracht.\n\nsummary(model2)\n\n\nCall:\nlm(formula = salary ~ yrs.service + rank + discipline + sex, \n    data = Salaries)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-64202 -14255  -1533  10571  99163 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    68351.7     4482.2   15.25  < 2e-16 ***\nyrs.service      -88.8      111.6   -0.80  0.42696    \nrankAssocProf  14560.4     4098.3    3.55  0.00043 ***\nrankProf       49159.6     3834.5   12.82  < 2e-16 ***\ndisciplineB    13473.4     2315.5    5.82  1.2e-08 ***\nsexMale         4771.2     3878.0    1.23  0.21931    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22700 on 391 degrees of freedom\nMultiple R-squared:  0.448, Adjusted R-squared:  0.441 \nF-statistic: 63.4 on 5 and 391 DF,  p-value: <2e-16\n\n\nZo is te zien dat het afkomstig zijn uit discipline B (toegepaste afdelingen) significant geassocieerd is met een gemiddelde stijging van 13473,38 in salaris in vergelijking met discipline A (theoretische afdelingen).\n\n\n\n\n\n\nOpmerking\n\n\n\nSommige categorische variabelen hebben geordende niveaus. Deze kunnen worden geconverteerd naar numerieke waarden, gerangschikt van laag naar hoog.\n\n\n\n\n\n\n1. ‘Fisher’s Exact Test’, Wikipedia, (2022).\n\n\n2. ‘Unusual Observations - Outlier, Leverage, and Influential Points’, https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A., ‘Statistiek om mee te werken’, Third dr., Stenfert Kroese, (1991).\n\n\n4. Chouldechova, A., ‘Regression Diagnostic Plots’, https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, ‘R Linear Regression Tutorial: Lm Function in R with Code Examples’, https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., ‘Finding a Suitable Linear Model for Ozone Prediction’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M., ‘Interpreting Linear Prediction Models’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, ‘Beginners Guide to Regression Analysis and Plot Interpretations’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, ‘Practical Guide to Logistic Regression Analysis in R’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A., ‘Logistic Regression Essentials in R’, 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., ‘Machine Learning Essentials: Practical Guide in R’, 1st edition dr., (2018).\n\n\n12. University of Virginia, ‘Understanding Diagnostic Plots for Linear Regression Analysis | University of Virginia Library Research Data Services + Sciences’, https://data.library.virginia.edu/diagnostic-plots/."
  },
  {
    "objectID": "correlatie.html#covariantie",
    "href": "correlatie.html#covariantie",
    "title": "4  Correlatie",
    "section": "4.1 Covariantie",
    "text": "4.1 Covariantie\nEen maatstaf voor de lineaire samenhang tussen twee kansvariabelen is de covariantie.\nVoor een populatie:\n\\[cov(x,y) = \\sigma_{xy} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\mu_x)(y_i-\\mu_y)\\]\nVoor een steekproef:\n\\[cov(x,y) = s_{xy} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})\\]\nHet interpreteren van de covariantie kan lastig zijn omdat deze niet gestandaardiseerd is. Standaardiseren wil zeggen dat je corrigeert voor de meetschaal van de variabele. Als bijvoorbeeld de massa van een voorwerp uitdrukt in kilogrammen, dan is de meetschaal duizend keer kleiner dan wanneer je de massa uitdrukt in grammen. De covariantie is afhankelijk van de meetschaal van de variabelen, en is dus groter als massa in grammen is uitgedrukt dan als massa in kilogrammen is uitgedrukt. Daarom wordt meestal met de correlatiecoëfficient gewerkt."
  },
  {
    "objectID": "correlatie.html#correlatiecoëfficient",
    "href": "correlatie.html#correlatiecoëfficient",
    "title": "4  Correlatie",
    "section": "4.2 Correlatiecoëfficient",
    "text": "4.2 Correlatiecoëfficient\nDoor de covariantie te delen door de standaarddeviaties corrigeer je wel voor de meetschalen. Daarom ligt de correlatiecoefficient altijd tussen -1 en +1. Het teken geeft de richting (positief of negatief) aan. Als er geen relatie bestaat tussen de twee variabelen is de correlatiecoëfficiënt 0.\nDe correlatie tussen twee variabelen kun je in R berekenen met de functie cor().\nsyntax: cor(x, y = NULL, method = c(\"pearson\", \"kendall\", \"spearman\"))\n\nx, een numerieke vector, matrix of data frame\ny, NULL (default, en is gelijk aan y=x) of een vector, matris of data frame, met vergelijkbare afmetingen als x.\nmethod, geeft aan welke correlatiecoëfficient berekend moet worden. Pearson is de meest gebruikte.\n\n\n4.2.1 Pearson\nGebruik de Pearson correlatiecoëfficiënt om de mate van lineaire samenhang te beoordelen tussen twee kwantitatieve (interval of ratio) variabelen. De aanwezigheid van uitbijters beinvloedt de maat.\nVoor een populatie:\n\\[\\rho_{xy} = \\frac{cov(x,y)}{\\sigma_x \\sigma_y} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\\]\nVoor een steekproef:\n\\[r(x,y) = \\frac{s_{xy}}{s_x s_y}\\]\n\ncor(mydf, method = \"pearson\")\n\n      x     y     z\nx 1.000 0.881 0.220\ny 0.881 1.000 0.192\nz 0.220 0.192 1.000\n\n\nOok hieruit blijkt dat er een sterke positieve correlatie is tussen x en y.\nHou bij het interpreteren van de Pearson correlatiecoefficiënt wel het volgende in gedachten:\n\nCorrelatie impliceert geen oorzakelijk verband.\nDeze is gevoelig voor uitschieters (uitbijters).\nHet zegt alleen iets over een mogelijke lineaire samenhang en niets over een niet-lineaire samenhang.\n\nWanneer de gegevensverzameling meerdere variabelen bevat, dan kun je correlatiecoefficiënt voor alle mogelijke combinaties van twee variabelen berekenen en deze afbeelden in een correlatiematrix.\nAls voorbeeld de berekening van de correlatiematrix voor mtcars.\n\n#Berekening correlatie(matrix)\ncormat <- round(cor(mtcars, method = \"pearson\"), 2) # op 2 decimalen\ncormat\n\n       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\nmpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55\ncyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53\ndisp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39\nhp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75\ndrat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09\nwt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43\nqsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66\nvs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57\nam    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06\ngear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27\ncarb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00\n\n\nMerk ook op dat de correlatiecoëfficiënten langs de diagonaal van de tabel allemaal gelijk zijn aan 1 omdat elke variabele uiteraard een perfecte correlatie met zichzelf heeft. Verder is de correlatiematrix symmetrisch. De waarden boven de diagonaal zijn gelijk aan de waarden onder de diagonaal. Vaak wordt deze bovenste helft dan ook weggelaten.\nMet package corrplot kun je de correlatiematrix visualiseren waardoor het beoordelen van de matrix gemakkelijker en sneller gaat.\n\ncorrplot::corrplot(cormat, \n                   title = \"Correlatiematrix mtcars\", \n                   type = \"lower\", \n                   tl.cex = 0.8, tl.col = \"black\", tl.srt = 45)\n\n\n\n\nPositieve waarden worden in blauw weergegeven en negatieve waarden in rood. De intensiteit van de kleur is evenredig met de correlatiecoëfficiënt, dus hoe sterker de correlatie (d.w.z. hoe dichter bij -1 of 1), hoe donkerder de vakken.\nEr zijn veel argumenten beschikbaar om de opmaak te wijzigen. Een voorbeeld\n\ncorrplot::corrplot(cormat, method = \"color\", type = \"upper\", outline = TRUE, \n                   diag = FALSE, addCoef.col = \"black\", number.digits = 2)\n\n\n\n\n\nmethod bepaalt de vorm van het correlatieobject. Mogelijke waarden: “circle” (default), “square”, “ellipse”, “number”, “pie”, “shade” en “color”.\ntype bepaalt wat weergegeven wordt. Mogelijke waarden: “full”, “upper” of “lower”.\noutline voor het weergeven van de zwarte omtrek bij cirkels, vierkanten, …\ndiag voor het weergeven van de correlatiecoefficienten op de hoofddiagonaal. Waarden: FALSE (default), TRUE\naddCoef.col om de correlatiecoefficienten weer te geven in een bepaalde kleur.\nnumber.digits het aantal decimalen\nsig.level het significantieniveau (default 0.05)\n\n\n\n4.2.2 Spearman\nGebruik de Spearman correlatiecoëfficient om de mate van samenhang te beoordelen tussen rangorde variabelen. Deze test kan worden gebruikt als de gegevens niet afkomstig zijn van een bivariate normale verdeling.\nAls voorbeeld de berekening van de correlatie tussen het aantal cylinders en het aantal versnellingen in de data mtcars.\n\ncormat <- cor(mtcars[, c(\"cyl\", \"gear\")], method = \"spearman\")\nround(cormat, 2)\n\n       cyl  gear\ncyl   1.00 -0.56\ngear -0.56  1.00\n\n\n\n\n4.2.3 Kendall\nDe Kendall correlatiecoëfficiënt kan ook gebruikt worden om de mate van samenhang te beoordelen tussen rangorde variabelen. Deze test kan worden gebruikt als de gegevens niet noodzakelijkerwijs afkomstig zijn van een bivariate normale verdeling. Het is echter een niet-parametrische maat."
  },
  {
    "objectID": "correlatie.html#correlatie-en-causaliteit",
    "href": "correlatie.html#correlatie-en-causaliteit",
    "title": "4  Correlatie",
    "section": "4.3 Correlatie en causaliteit",
    "text": "4.3 Correlatie en causaliteit\nCorrelatie is geen causaliteit. Het kan zijn dat er correlatie tussen twee variabelen is, maar dat een andere variabele de samenhang verklaart. Een bekend voorbeeld: er is een relatie tussen schoenmaat van kinderen en hoe goed ze kunnen lezen. Maar dat betekent niet dat je schoenmaat je leesniveau beïnvloedt (of, gekker nog, dat je grotere voeten krijgt als je goed kunt lezen). Nee, er is een derde ‘verstorende variabele’: leeftijd. Oudere kinderen hebben én grotere voeten én kunnen beter lezen.\nVoor causaliteit moet je onderzoeksmodel voldoen aan een aantal voorwaarden; (1) correlationeel verband, (2) tijdsvolgorde\u000b, eerst oorzaak dan gevolg, en (3) afwezigheid vs aanwezigheid oorzaak.\nMooie voorbeelden van valse correlaties vind je op Spurious correlations"
  },
  {
    "objectID": "correlatie.html#correlatie-toetsing",
    "href": "correlatie.html#correlatie-toetsing",
    "title": "4  Correlatie",
    "section": "4.4 Correlatie toetsing",
    "text": "4.4 Correlatie toetsing\nMet de functie cor.test kun je een correlatietoetsing uitvoeren.\ncor.test(x, y, alternative, method, conf.level, ...)\n\nalternativegeeft de alternatieve hypothese aan. Waarden: “two.sided”, “greater”, “less”.\nmethod bepaalt de te gebruiken methode. Waarden: “pearson”, “kendall”, “spearman”\nconf.level bepaalt het betrouwbaarheidsinterval (alleen voor Pearson)\n\nDe hypotheses zijn:\n\n\\(H_0: \\rho = 0\\) (correlatiecoefficient =0, er is geen correlatie)\n\\(H_1: \\rho \\ne 0\\)\n\n\n\n\n\n\n\nOpmerking\n\n\n\nBeslissingsregel: Als de p-waarde < alpha (0.05) dan wordt de nulhypothese verworpen.\n\n\nAls voorbeeld wordt de ingebouwde dataset mtcars gebruikt. De samenhang tussen de variabelen mpg en wt wordt nu onderzocht.\nPearson\n\ncor.test(mtcars$mpg, mtcars$wt, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$wt\nt = -10, df = 30, p-value = 1e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.934 -0.744\nsample estimates:\n   cor \n-0.868 \n\n\nDe p-waarde is kleiner dan \\(\\alpha = 0,05\\). Je kunt dus concluderen dat wt en mpg significant correleren met een correlatiecoëfficient van -0.87 en een p waarde van 1.29410^{-10}\nKendall\n\ncor.test(mtcars$mpg, mtcars$wt, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  mtcars$mpg and mtcars$wt\nz = -6, p-value = 7e-09\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n   tau \n-0.728 \n\n\nSpearman\n\ncor.test(mtcars$mpg, mtcars$wt, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  mtcars$mpg and mtcars$wt\nS = 10292, p-value = 1e-11\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n   rho \n-0.886 \n\n\nVoor de visualisatie kun je ggscatter() uit package ggpubr gebruiken.\n\nggpubr::ggscatter(mtcars, x = \"mpg\", y = \"wt\", \n          add = \"reg.line\", conf.int = TRUE, \n          cor.coef = TRUE, cor.method = \"pearson\", \n          xlab = \"Miles/gallon (us)\", ylab = \"Weight (1000 lbs)\")\n\n\n\n\nFiguur 4.1: Spreidingsdiagram\n\n\n\n\n\n4.4.1 Controle aannames\n1 - Is de covariantie lineair?\nJa, gezien de grafiek. In de situatie waarin de spreidingsdiagrammen gebogen patronen vertonen, heb je te te maken met niet-lineaire associatie tussen de twee variabelen.\n2 - Volgen de gegevens van elk van de twee variabelen een normale verdeling?\nGebruik Shapiro-Wilk normaliteitstest.\n\nshapiro.test(mtcars$mpg) # Shapiro-Wilk normality test voor mpg\n\n\n    Shapiro-Wilk normality test\n\ndata:  mtcars$mpg\nW = 0.9, p-value = 0.1\n\nshapiro.test(mtcars$wt)  # Shapiro-Wilk normality test voor wt\n\n\n    Shapiro-Wilk normality test\n\ndata:  mtcars$wt\nW = 0.9, p-value = 0.09\n\n\nDe twee p-waarden zijn groter dan het significantieniveau 0,05, wat betekent dat de verdeling van de gegevens niet significant verschilt van de normale verdeling. Met andere woorden, we kunnen uitgaan van de normaliteit.\nBeoordeel de normaliteitsgrafiek.\n\nggpubr::ggqqplot(mtcars$mpg, ylab = \"mpg\")\nggpubr::ggqqplot(mtcars$wt, ylab = \"wt\")\n\n\n\n\n\n\n\n(a) mpg\n\n\n\n\n\n\n\n(b) wt\n\n\n\n\nFiguur 4.2: Normaliteitsgrafieken\n\n\n\nUit deze verdelingen kun je concluderen dat beide populaties uit normale verdelingen komen.\n\n\n\n\n\n\nOpmerking\n\n\n\nWanneer de gegevens niet normaal worden verspreid, dan is het aan te bevelen om de niet-parametrische correlatie te gebruiken, inclusief de op rang gebaseerde correlatietests van Spearman en Kendall.\n\n\n\n\n\n\n1. ‘Fisher’s Exact Test’, Wikipedia, (2022).\n\n\n2. ‘Unusual Observations - Outlier, Leverage, and Influential Points’, https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A., ‘Statistiek om mee te werken’, Third dr., Stenfert Kroese, (1991).\n\n\n4. Chouldechova, A., ‘Regression Diagnostic Plots’, https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, ‘R Linear Regression Tutorial: Lm Function in R with Code Examples’, https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., ‘Finding a Suitable Linear Model for Ozone Prediction’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M., ‘Interpreting Linear Prediction Models’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, ‘Beginners Guide to Regression Analysis and Plot Interpretations’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, ‘Practical Guide to Logistic Regression Analysis in R’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A., ‘Logistic Regression Essentials in R’, 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., ‘Machine Learning Essentials: Practical Guide in R’, 1st edition dr., (2018).\n\n\n12. University of Virginia, ‘Understanding Diagnostic Plots for Linear Regression Analysis | University of Virginia Library Research Data Services + Sciences’, https://data.library.virginia.edu/diagnostic-plots/."
  },
  {
    "objectID": "voorwaarden.html#lineaire-relatie",
    "href": "voorwaarden.html#lineaire-relatie",
    "title": "5  Voorwaarden",
    "section": "Lineaire relatie",
    "text": "Lineaire relatie\nEr is een lineaire relatie tussen de afhankelijke variabele \\(y\\) en de onafhankelijke variabele \\(x\\).\nDit kun je het eenvoudigst beoordelen door het spreidingsdiagram te bekijken. Als het erop lijkt dat de punten in de plot langs een rechte lijn zouden kunnen vallen, dan bestaat er een soort lineaire relatie tussen de twee variabelen en is aan deze voorwaarde voldaan.\nWanneer niet aan deze voorwaarde voldaan wordt dan heb je de volgende opties:\n\nPas een transformatie toe op \\(x\\) of \\(y\\). Veel gebruikte transformaties zijn: logaritme, vierkantswortel, reciproke.\nVoeg nog een andere onafhankelijke variabele aan het model toe. Bijvoorbeeld wanneer het spreidingsdiagram een paraboolvorm heeft zou je kunnen proberen om \\(x^2\\) als extra onafhankelijke variabele toe te voegen."
  },
  {
    "objectID": "voorwaarden.html#onafhankelijkheid",
    "href": "voorwaarden.html#onafhankelijkheid",
    "title": "5  Voorwaarden",
    "section": "Onafhankelijkheid",
    "text": "Onafhankelijkheid\nDe residuen zijn onafhankelijk van elkaar (geen autocorrelatie).\nEr mag geen patroon zitten in opeenvolgende waarden van de residuen. Dit is eigenlijk alleen maar relevant bij het werken met tijdreeksen, waar je immers met opeenvolgende waarden te maken hebt.\nJe kunt dit het eenvoudigst beoordelen door een residuendiagram te beoordelen. Er bestaan formele toetsen voor deze voorwaarde zoals de Durbin-Watson test (zie Hoofdstuk 2)\nWanneer er sprake is van autocorrelatie kun je de gegevens differentieren."
  },
  {
    "objectID": "voorwaarden.html#homoscedasticiteit",
    "href": "voorwaarden.html#homoscedasticiteit",
    "title": "5  Voorwaarden",
    "section": "Homoscedasticiteit",
    "text": "Homoscedasticiteit\nDe residuen hebben een constante (dezelfde) variantie voor elke waarde van x. Dit heet ook wel homoscedasticiteit. Wanneer dat niet het geval is dan is er spraken van heteroscedasticiteit.\nWanneer heteroscedasticiteit aanwezig is in een regressieanalyse, worden de resultaten van de analyse moeilijk te vertrouwen. Specifiek verhoogt heteroscedasticiteit de variantie van de schattingen van de regressiecoëfficiënt, maar het regressiemodel pikt dit niet op. Hierdoor kan het voorkomen dat een regressiemodel aangeeft dat een term in het model statistisch significant is, terwijl dat in feite niet zo is.\nJe kunt dit het eenvoudigst beoordelen door in een spreidingsdiagram de residuen uit te zetten tegen de geschatte waarden. Bij heteroscedasticiteit worden de residuen steeds meer verspreid naarmate de geschatte waarden groter worden. Het patroon heeft de vorm van een kegel.\n\n\n\nheteroscedasticiteit\n\n\nEr zijn een aantal manieren om te proberen de heteroscedasticiteit te laten verdwijnen:\n\nTransformeren van de afhankelijke variabele door de logaritme hiervan te nemen.\nHerdefinitie van de afhankelijke variabele. Dit kan niet altijd. Maar bijvoorbeeld in plaats van absolute aantallen besmette personen zou je het aantal besmette personen per 100.000 inwoners kunnen nemen, zoals bij COVID-19 vaak gebeurt.\nGebruik gewogen regressie. Dit type regressie kent een gewicht toe aan elk gegevenspunt op basis van de variantie van de geschatte waarde. In wezen geeft dit kleine gewichten aan datapunten met hogere varianties, waardoor hun kwadratische residuen kleiner worden. Als de juiste gewichten worden gebruikt, kan dit het probleem van heteroscedasticiteit elimineren."
  },
  {
    "objectID": "voorwaarden.html#normaliteit",
    "href": "voorwaarden.html#normaliteit",
    "title": "5  Voorwaarden",
    "section": "Normaliteit",
    "text": "Normaliteit\nDe residuen zijn normaal verdeeld (dus toevallige waarden).\nDeze voorwaarde kun je op een van de volgende manieren controleren:\n\nBestudeer de verdeling grafisch: histogram, boxplot, QQ plot\nAnalyseer scheefheid en kurtosis\nGebruik statistische toetsen als Chi-kwadraat, Shapiro-Wilk en Kolmogorov-Smironov.\n\nWanneer niet aan deze voorwaarde voldaan wordt kun je het volgende doen:\n\nGa na of er uitschieters zijn met een grote impact op de verdeling. Zoja, onderzoek dan of dit wil echte waarden zijn en geen foute invoerwaarden.\nPas een transformatie toe op \\(x\\) of \\(y\\). Veel gebruikte transformaties zijn: logaritme, vierkantswortel, reciproke."
  },
  {
    "objectID": "voorwaarden.html#verbetertips",
    "href": "voorwaarden.html#verbetertips",
    "title": "5  Voorwaarden",
    "section": "Verbetertips",
    "text": "Verbetertips\n\nAls de gegevens niet lineair gedrag vertonen kun je de onafhankelijke variabelen transformeren met wortel, kwadraat, logaritme.\nAls de gegevens lijden aan heteroskedasticiteit, transformeer dan de afhankelijke variabele met behulp van wortel, kwadraat, logaritme, enz. je kunt ook de gewogen kleinste-kwadratenmethode gebruiken om dit probleem aan te pakken.\nAls de gegevens multicollineariteit vertonen, gebruik dan een correlatiematrix om gecorreleerde variabelen te controleren. Laten we zeggen dat variabelen A en B sterk gecorreleerd zijn. Gebruik nu deze benadering in plaats van er één te verwijderen: zoek de gemiddelde correlatie van A en B met de rest van de variabelen. Welke variabele het hoogste gemiddelde heeft in vergelijking met andere variabelen, verwijder deze. Als alternatief kun je bestrafte regressiemethoden gebruiken, zoals lasso, ridge, elastic net, enz.\nje kunt variabelen selecteren op basis van p-waarden. Als een variabele een p-waarde > 0,05 vertoont, kun je die variabele uit het model verwijderen, omdat we bij p> 0,05 altijd de nulhypothese niet verwerpen.\n\n\n\n\n\n1. ‘Fisher’s Exact Test’, Wikipedia, (2022).\n\n\n2. ‘Unusual Observations - Outlier, Leverage, and Influential Points’, https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A., ‘Statistiek om mee te werken’, Third dr., Stenfert Kroese, (1991).\n\n\n4. Chouldechova, A., ‘Regression Diagnostic Plots’, https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, ‘R Linear Regression Tutorial: Lm Function in R with Code Examples’, https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., ‘Finding a Suitable Linear Model for Ozone Prediction’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M., ‘Interpreting Linear Prediction Models’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, ‘Beginners Guide to Regression Analysis and Plot Interpretations’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, ‘Practical Guide to Logistic Regression Analysis in R’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A., ‘Logistic Regression Essentials in R’, 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., ‘Machine Learning Essentials: Practical Guide in R’, 1st edition dr., (2018).\n\n\n12. University of Virginia, ‘Understanding Diagnostic Plots for Linear Regression Analysis | University of Virginia Library Research Data Services + Sciences’, https://data.library.virginia.edu/diagnostic-plots/."
  },
  {
    "objectID": "overfitting.html",
    "href": "overfitting.html",
    "title": "6  Over- en underfitting",
    "section": "",
    "text": "Bron: Learning Data Science: Modelling Basics\nBij data science gaat het vaak om het maken van goede modellen. Bijvoorbeeld, kun je op basis van iemands leeftijd zijn maandsalaris voorspellen? Hierbij een voorbeeld met gefingeerde data. In dit model is leeftijd de onafhankelijke variabele en salaris de afhankelijke variabele.\nZie ook Excel bestand: leeftijd-inkomen.xlsx\n\nmijndata <- tibble(leeftijd = c(21, 46, 55, 35, 28), \n                   salaris = c(1850, 2500, 2560, 2230, 1800))\n\np <- ggplot(mijndata, aes(x = leeftijd, y = salaris)) + geom_point()\np\n\n\n\n\nFiguur 6.1: Salaris - Leeftijd.\n\n\n\n\nIn de grafiek is een zekere lineaire afhankelijkheid waar te nemen. Met de functie cor kun je de correlatiecoefficient bepalen welke hier dicht bij 1 ligt.\n\ncor(mijndata)\n\n         leeftijd salaris\nleeftijd    1.000   0.946\nsalaris     0.946   1.000\n\n\nregressielijn\nEr wordt een lineair regressiemodel gemaakt\n\np + geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, col = \"green\")\n\n\n\n\nFiguur 6.2: Regressielijn voor het lineaire model.\n\n\n\n\nBeoordeling model\n\nmodel <- lm(salaris ~ leeftijd, data = mijndata)\nsummary(model)\n\n\nCall:\nlm(formula = salaris ~ leeftijd, data = mijndata)\n\nResiduals:\n     1      2      3      4      5 \n  54.9   91.0  -70.0   91.1 -167.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  1279.37     188.51    6.79   0.0065 **\nleeftijd       24.56       4.84    5.08   0.0148 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 132 on 3 degrees of freedom\nMultiple R-squared:  0.896, Adjusted R-squared:  0.861 \nF-statistic: 25.8 on 1 and 3 DF,  p-value: 0.0148\n\n\nDe vergelijking van de lijn is dus \\(salaris = 1279.367 + 24.588 \\times leeftijd\\)\nDe p-waarden in de laatste kolom laten zien dat beide parameters significant zijn.\nVoorspelling salaris van een 20-jarige wordt \\(1279.367 + 24.588 \\times 20 = 1770.52\\). Dit kan ook in R met de functie predict(). Hierbij moet je de onafhankelijke variabele met zijn naam als data frame gebruiken, anders krijg je een foutmelding. En je kunt ook voor meerdere waarden tegelijk een voorspelling maken.\n\npredict(model, data.frame(leeftijd = 20))\n\n   1 \n1771 \n\npred_model <- predict(model, data.frame(leeftijd = seq(0, 80, 5)))\nnames(pred_model) <- seq(0, 80, 5)\npred_model\n\n   0    5   10   15   20   25   30   35   40   45   50   55   60   65   70   75 \n1279 1402 1525 1648 1771 1893 2016 2139 2262 2384 2507 2630 2753 2876 2998 3121 \n  80 \n3244 \n\n\npolynoom regressiemodel\nWat nu als je een model maakt dat nog nauwkeuriger is, een model waarbij de lijn daadwerkelijk alle gegevenspunten doorloopt, zodat alle beschikbare informatie wordt gebruikt. Dat kan met 4e-graads polynoom.\n\np + geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ poly(x, 4), col = \"red\")\n\n\n\n\nFiguur 6.3: Polynoom regressiemodel.\n\n\n\n\n\nPolyReg <- lm(salaris ~ poly(leeftijd, 4, raw = TRUE), data = mijndata)\n\npred_PolyReg <- predict(PolyReg, data.frame(leeftijd = seq(0, 80, 5)))\nnames(pred_PolyReg) <- seq(0, 80, 5)\npred_PolyReg\n\n    0     5    10    15    20    25    30    35    40    45    50    55    60 \n19527 11104  5955  3164  1959  1706  1912  2230  2450  2505  2469  2560  3134 \n   65    70    75    80 \n 4690  7868 13451 22362 \n\n\nIs dit een beter model?\nDe lijn vertoont een raar verloop. Zo kun je geen goede reden bedenken waarom het salaris zou stijgen naarmate je jonger of ouder bent, evenals waarom tussen 21 en 28 jaar en tussen 46 en 55 jaar een buil is. Dit wordt veroorzaakt doordat het model de ruis in de data als goede invoer gebruikt. Het model wordt daardoor eigenlijk slechter!\nOm een goed model te hebben, heb je altijd wat rigiditeit nodig om de gegevens te generaliseren. Dit voorbeeld illustreert een van de grote problemen van machine learning: overfitting van de gegevens. Overfitting betekent dat het model niet alleen past bij de systematische gegevens, maar ook bij de ruis. Een model met overfitting zal daarom waarschijnlijk slecht presteren bij nieuwe gegevens.\nHet andere uiterste is onderfitting. Dit kan gedemonstreerd worden door het gemiddelde van salaris te gebruiken als model. Hierbij krijgt iedereen een salaris van 2188 en is met de blauwe lijn aangegeven.\n\np + \n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, col = \"green\") +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ poly(x, 4), col = \"red\") + \n    geom_hline(yintercept = mean(mijndata$salaris), col = \"blue\") + \n    ggtitle(\"Alle modellen zijn niet goed, sommige zijn bruikbaar\")\n\n\n\n\nFiguur 6.4: Blauw = Gemiddelde (onderfitting), Groen = Lineaire fit, Rood = Polynoom fit (overfitting).\n\n\n\n\nEen ander belangrijk punt is dat alle modellen hun toepassingsgebied hebben. In dit geval waren er geen gegevens voor mensen van zeer jonge of zeer hoge leeftijd, dus het is over het algemeen erg gevaarlijk om voorspellingen te doen voor die mensen, dat wil zeggen voor waarden buiten het oorspronkelijke observatiebereik (ook wel extrapolatie genoemd - in tegenstelling tot interpolatie = voorspellingen binnen het oorspronkelijke bereik). Het lineaire model geeft in dit voorbeeld zelfs een salaris van 1279.367 aan een pas geborene! En dat het salaris steeds maar toeneemt naarmate de mensen ouder wordt slaat ook nergens op.\n\n\n\n\n1. ‘Fisher’s Exact Test’, Wikipedia, (2022).\n\n\n2. ‘Unusual Observations - Outlier, Leverage, and Influential Points’, https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A., ‘Statistiek om mee te werken’, Third dr., Stenfert Kroese, (1991).\n\n\n4. Chouldechova, A., ‘Regression Diagnostic Plots’, https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, ‘R Linear Regression Tutorial: Lm Function in R with Code Examples’, https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., ‘Finding a Suitable Linear Model for Ozone Prediction’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M., ‘Interpreting Linear Prediction Models’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, ‘Beginners Guide to Regression Analysis and Plot Interpretations’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, ‘Practical Guide to Logistic Regression Analysis in R’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A., ‘Logistic Regression Essentials in R’, 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., ‘Machine Learning Essentials: Practical Guide in R’, 1st edition dr., (2018).\n\n\n12. University of Virginia, ‘Understanding Diagnostic Plots for Linear Regression Analysis | University of Virginia Library Research Data Services + Sciences’, https://data.library.virginia.edu/diagnostic-plots/."
  },
  {
    "objectID": "logitmodel.html#inleiding",
    "href": "logitmodel.html#inleiding",
    "title": "7  Logistisch model",
    "section": "7.1 Inleiding",
    "text": "7.1 Inleiding\nLogistische regressie is net als bij lineaire regressie een methode om een verband tussen een afhankelijke variabele en een of meer onafhankelijke varabelen te modelleren.\nBij lineaire regressie is de afhankelijke variabele kwantitatief, interval/ratio geschaald. Bij logistische regressie is de afhankelijke variabele kwalitatief, een categorie dus. Wanneer deze variabele maar twee uitkomsten heeft, dan is het een binaire of dichotome variabele.\nHet voorspellen van de waarde voor een kwalitatieve variabele kan worden aangeduid als classificeren, het indelen in een categorie of klasse. Aan de andere kant proberen de methodes die vaak voor classificatie gebruikt worden eerst de kansen voor elke mogelijke categoriewaarde te voorspellen, als basis voor het maken van de classificatie.\nHet logistische model gaat uit van kansen, of beter gezegd van kansverhoudingen: odds. Dit is de kans op de ene uitkomst gedeeld door de kans op de andere uitkomst. Een odds heeft een bereik van 0 tot oneindig.\n\n\n\n\n\n\nOpmerking\n\n\n\nSynoniemen voor logistische regressie zijn binaire logistische regressie, binomiale logistische regressie, logit model."
  },
  {
    "objectID": "logitmodel.html#logistisch-model",
    "href": "logitmodel.html#logistisch-model",
    "title": "7  Logistisch model",
    "section": "7.2 Logistisch model",
    "text": "7.2 Logistisch model\nIn Figuur 7.1 (a) zie je het verband tussen het aantal minuten dat je aan huiswerk wiskunde besteed hebt en de slaagkans voor de wiskundetoets.\nDe kans om voor de wiskunde toets te slagen is aanvankelijk nogal laag, die zit heel dicht bij nul. Dus als je heel weinig tijd aan je huiswerk besteed dan slaag je niet voor deze wiskundetoets. Die slaagkans gaat bij ca. 30 min. huiswerk stijgen en vanaf 40 min zie je de slaagkans vrij lineair en snel omhoog gaan totdat je ongeveer bij 70 min. bent en dan is de kans dat je die wiskundetoets haalt bijna gelijk aan 100%. Dit is de beroemde S-curve vanwege de vorm.\n\n\n\n\n\n\n\n(a) Slaagkans Wiskunde - huiswerktijd\n\n\n\n\n\n\n\n(b) Slaagkans Wiskunde - huiswerktijd met lineaire trendlijn\n\n\n\n\nFiguur 7.1: Verband tussen het aantal minuten dat je aan huiswerk wiskunde besteed hebt en de slaagkans voor de wiskundetoets.\n\n\nDat je het verband tussen slaagkans en huiswerk niet kunt modelleren met gewone lineaire regressie is te zien in Figuur 7.1 (b), waar een lineaire trendlijn is aangebracht. Hierin zijn een aantal opmerkelijke zaken te zien:\n\nWanneer je geen huiswerk maakt (huiswerk = 0), dan is er een negatieve kans. Dit kan natuurlijk niet.\nDe kans om de wiskundetoets te halen kan groter dan 1 zijn. Dit kan evenmin.\nIn de eerste helft van de grafiek geeft de trendlijn een overschatting van de kans.\nIn de tweede helft van de grafiek geeft de trendlijn een onderschatting van de kans.\nDe trendlijn heeft een intercept van -0,229 (negatief!) en een richtingscoefficient (rc) van 0,014\n\nDeze regressielijn past niet bij de werkelijke observaties. Dus de gewone lineaire regressie is niet geschikt om dit soort kansen te modelleren.\nBij de logistische regressie wordt een speciale kanscurve gebruikt, waarbij de geschatte kans nooit onder de nul komt en ook nooit groter dan 1 wordt. Dat maakt deze kanscurve zeer geschikt om variabelen te analyseren die alleen 0 en 1 kennen als waarde. Bij een toets zou dit kunnen zijn:\n\n0 = toets niet gehaald\n1 = toets wel gehaald\n\nDe uitkomst van een logistische regressie is niet direct de categoriewaarde (hier 0 of 1), maar de kans op een bepaalde categoriewaarde. Deze kans ligt tussen 0 en 1. Het is aan de onderzoeker om op basis van die kans aan te geven in welke categorie de waarneming valt, dus wat het omkeerpunt van de kans voor categorie 0 of categorie 1 is. Standaard ligt het omslagpunt bij een kans van 50%.\nHet logistische model gaat uit van kansverhoudingen: odds\nIn het voorbeeld is \\(\\text{odds} = \\frac{p_{wel}} {p_{niet}} = \\frac{p_{wel}} {1 - p_{wel}}\\)\n\\(p_{wel}\\) heeft een bereik van \\([0, 1]\\) waardoor de odds een bereik heeft van \\([0, \\infty]\\).\nOmdat het niet handig is te werken met een variabele die naar oneindig loopt, wordt de natuurlijke logaritme van de odds genomen. Deze wordt de log odds of logit genoemd.\nWanneer de onafhankelijke variabelen \\(X_1, X_2, X_3, ...\\) zijn, dan heeft het logistische model de volgende vorm:\n\\[ln(odds) = ln(\\frac{p_{wel}} {p_{niet}}) = b_0 + b_1X_1 + b_2X_2 + ...\\]\nDit lijkt veel op een gewoon regressiemodel: \\(b_0\\) is het intercept en \\(b_1, b_2, ...\\) zijn de parameters die het effect van \\(X_1, X_2, ...\\) aangeven.\nEn geschreven als kansen\n\\[\\text{odds} =\\frac{p_{wel}} {p_{niet}} = e^{(b_0 + b_1X_1 + b_2X_2 + ...)}\\]\n\\[p_{wel} = \\frac{e^{(b_0 + b_1X_1 + b_2X_2 + ...)}}{e^{(b_0 + b_1X_1 + b_2X_2 + ...)} + 1}\\]\n\\[p_{niet} = \\frac{1}{e^{(b_0 + b_1X_1 + b_2X_2 + ...)} + 1}\\]\nDe kansen \\(p_{wel}\\) en \\(p_{niet}\\) zijn dus afhankelijk van de variabelen \\(X_1, X_2, ...\\), maar deze afhankelijkheid is niet lineair. De regressielijn heeft de vorm van een S-curve.\n\n\n\n\n\n\n\nNegatief effect X\n\n\n\n\n\n\n\nPositief effect X\n\n\n\n\nFiguur 7.2: S-curves voor positieve en negatieve effecten.\n\n\n\n7.2.1 Odds Ratio\nDe odds ratio is de verhouding tussen twee odds.\nVoorbeeld. In een mand zitten 4 rode ballen en 1 groene bal. Je haalt nu 1 bal uit de mand.\n\\(P_{rood} = 4/5 = 0,8; P_{groen}=1/5=0,2\\)\n\\(Odds_{rood} = P_{rood}/P_{niet rood} = 0,8/0,2 = 4\\)\n\\(Odds_{groen} = P_{groen}/P_{niet groen} = 0,2/0,8 = 0.25\\)\nDe odds ratio voor het trekken van een rode bal vergeleken met een groene bal is \\(Odds_{rood} / Odds_{groen} = 4 / 0,25 =16\\).\nDus de odds voor het trekken van een rode bal is 16 keer groter dan de odds voor het trekken van een groen bal.\nOnderzoekers gebruiken odds ratio’s in verschillende situaties wanneer ze de kansen willen analyseren van twee gebeurtenissen die kunnen plaatsvinden.\nEen veelvoorkomend voorbeeld is het bepalen of een nieuwe behandeling de kansen van een patiënt op een goed resultaat vergroot in vergelijking met een bestaande behandeling.\nTabel 7.1 geeft het aantal patiënten weer dat een gunstig of slecht gezondheidsresultaat had als gevolg van hun medicijn.\n\n\nTabel 7.1: Resultaten medicijnbehandeling\n\n\nBehandeling\nPositief resultaat\nNegatief resultaat\n\n\n\n\nNieuw medicijn\n60\n40\n\n\nBestaande behandeling\n42\n58\n\n\n\n\nVoor patienten die met het nieuwe medicijn behandeld zijn is \\(Odds_{positief} = 0,60/0,40 = 1,5\\)\nVoor patienten die met de bestaande behandeling is \\(Odds_{positief} = 0,42/0,58 = 0,7241379\\)\nOdds ratio voor nieuw medicijn: \\(Odds Ratio = 1,5 / 0,7241379 = 2.071429\\)\nDus de odds op een positief resultaat met het nieuwe medicijn is 2 keer groter dan voor de bestaande behandeling."
  },
  {
    "objectID": "logitmodel.html#voorbeeld-wiskundetoets",
    "href": "logitmodel.html#voorbeeld-wiskundetoets",
    "title": "7  Logistisch model",
    "section": "7.3 Voorbeeld wiskundetoets",
    "text": "7.3 Voorbeeld wiskundetoets\nBronnen Manfred te Grotenhuis (Radboud Universiteit):\n\nDeel 1: Inleiding logistische regressie-analyse\nDeel 2: Multivariate logistische regressie\n\nAls databestand wordt het SPSS bestand wiskunde.sav gebruikt, beschikbaar gesteld door Manfred te Grotenhuis. Dit zijn echte data, afkomstig uit de V.S.\n\n# Lees data in en converteer value labels naar factors\nlibrary(haven)\nwiskunde <- read_sav(\"data/wiskunde.sav\") %>% \n    as_factor(only_labelled = TRUE)\n\n# Inspectie van de data\nstr(wiskunde)\n\ntibble [8,834 × 7] (S3: tbl_df/tbl/data.frame)\n $ studentnummer: num [1:8834] 5 76 95 47 37 62 48 18 66 81 ...\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n $ sexe         : Factor w/ 2 levels \"man\",\"vrouw\": 2 2 1 2 2 1 1 2 2 2 ...\n $ etniciteit   : Factor w/ 5 levels \"Aziatisch\",\"Zuid-Amerikaans\",..: 4 4 4 4 4 4 3 4 2 2 ...\n $ huiswerk     : num [1:8834] 7 7 7 7 7 7 7 7 7 7 ...\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n  ..- attr(*, \"display_width\")= int 15\n $ schooltype   : Factor w/ 4 levels \"publiek\",\"prive katholiek\",..: 1 1 3 2 1 1 1 1 1 2 ...\n $ wiskundetoets: num [1:8834] 47.8 67.7 56 56.1 64.2 ...\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n $ voldoende    : Factor w/ 2 levels \"wiskundetoets niet gehaald\",..: 1 2 2 2 2 1 2 2 2 1 ...\n\nhead(wiskunde)\n\n# A tibble: 6 × 7\n  studentnummer sexe  etniciteit huiswerk schooltype           wiskund…¹ voldo…²\n          <dbl> <fct> <fct>         <dbl> <fct>                    <dbl> <fct>  \n1             5 vrouw blank             7 publiek                   47.8 wiskun…\n2            76 vrouw blank             7 publiek                   67.7 wiskun…\n3            95 man   blank             7 prive andere religie      56.0 wiskun…\n4            47 vrouw blank             7 prive katholiek           56.1 wiskun…\n5            37 vrouw blank             7 publiek                   64.2 wiskun…\n6            62 man   blank             7 publiek                   40.5 wiskun…\n# … with abbreviated variable names ¹​wiskundetoets, ²​voldoende\n\n\nHet al dan niet slagen voor de wiskundetoets afgezet tegen sexe.\n\ntable(wiskunde$voldoende, wiskunde$sexe)\n\n                            \n                              man vrouw\n  wiskundetoets niet gehaald 2525  2769\n  wiskundetoets WEL gehaald  1835  1705\n\n\nEen betere bij het college van Manfred aansluitende kruistabel krijg je met\nlibrary(gmodels)\nCrossTable(wiskunde$voldoende, wiskunde$sexe, digits = 1, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE, format = c(\"SPSS\"))\n\nlibrary(janitor)\nwiskunde %>%\n    tabyl(voldoende, sexe) %>%\n    adorn_totals(where = c(\"row\",\"col\")) %>%\n    adorn_percentages(denominator = \"col\") %>%\n    adorn_pct_formatting(digits = 1) %>% \n    adorn_ns(position = \"front\")\n\n                  voldoende           man         vrouw         Total\n wiskundetoets niet gehaald 2525  (57.9%) 2769  (61.9%) 5294  (59.9%)\n  wiskundetoets WEL gehaald 1835  (42.1%) 1705  (38.1%) 3540  (40.1%)\n                      Total 4360 (100.0%) 4474 (100.0%) 8834 (100.0%)\n\n\nCodering\nMet de codering\n\nvoldoende\n\n0: wiskundetoets NIET gehaald\n1: wiskundetoets wel gehaald\n\nsexe\n\n0: man\n1: vrouw\n\n\nJe kunt nu een odds uitrekenen voor de mannen en een odds voor de vrouwen\n\\(\\text{odds(mannen)} = \\frac{p1}{p0} = \\frac{0,421}{0,579} = 0,727\\)\nBij de mannen zijn naar verhouding MINDER voldoendes dan onvoldoendes.\n\\(\\text{odds(vrouwen)} = \\frac{p1}{p0} = \\frac{0,381}{0,619} = 0,616\\)\nBij de vrouwen zijn er naar verhouding nog minder voldoendes dan onvoldoendes vergeleken met de mannen.\n\\(\\text{odds ratio}= \\frac{odds(vrouwen)}{odds(mannen)} = \\frac{0,616}{0,727} = 0,847\\)\nDit getal geeft in feite het effect aan van het geslacht op het halen van de wiskundetoets. Als de oddsratio = 1 dan betekent dit dat zowel onder mannen als vrouwen de verhouding voldoende/onvoldoende gelijk is. Hier is de ratio kleiner dan 1, wat betekent dat onder de vrouwen minder voldoendes zijn dan onder de mannen. Met één getal, de odds ratio, kun je dus aangeven wat het effect is van het geslacht op het halen van de wiskundetoets.\nDe logistische regressie-analyse kun je uitdrukken in termen van odds, maar ook in termen van logitparameters. Het enige wat je hoeft te doen is de natuurlijke logaritme te berekenen van de odds en de oddsratio.\n\nmannen: ln(0,727) = -0,319\nvrouwen: ln(0,616) = -0,485\nln(0,847) = -0,166\n\nSamengevat\n\nmannen: odds = 0,727 en Logitparameter = -0,319\nvrouwen odds = 0,616 en Logitparameter = -0,485\noddsratio (vrouwen/mannen) = 0,847 en Logitparameter = -0,166\n\nMerk op dat de drie logitparameters negatief zijn.\nEen deel van de getallen zie je terug bij de logistische regressie-analyse met R.\n\nlogitmodel <- glm(voldoende ~ sexe, data = wiskunde, family = \"binomial\")\nsummary(logitmodel)\n\n\nCall:\nglm(formula = voldoende ~ sexe, family = \"binomial\", data = wiskunde)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -1.05   -1.05   -0.98    1.32    1.39  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.3192     0.0307  -10.41  < 2e-16 ***\nsexevrouw    -0.1657     0.0435   -3.81  0.00014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11896  on 8833  degrees of freedom\nResidual deviance: 11881  on 8832  degrees of freedom\nAIC: 11885\n\nNumber of Fisher Scoring iterations: 4\n\n\nKolom “Estimate” is de kolom met de logitparameters.\nJe kunt deze coéfficienten ook uit de list halen met de functie coef().\n\ncoef(logitmodel)\n\n(Intercept)   sexevrouw \n     -0.319      -0.166 \n\n\nEn door hiervan de e-machten te nemen krijg je de odds.\n\nexp(coef(logitmodel))\n\n(Intercept)   sexevrouw \n      0.727       0.847 \n\n\nDat in de regel met de intercept de odds voor mannen staat en niet die voor vrouwen komt omdat de mannen gecodeerd zijn met de waarde 0. Net als bij gewone regressie is de intercept bepaald voor x=0. De regel voor intercept heeft daarom betrekking op mannen.\n0,847 is het effect van de variabele geslacht en dit effect is significant. In die zin dat mannen de toets iets gemakkelijker halen dan de vrouwen.\nAls je de codes van mannen en vrouwen omdraait, dus vrouwen met 0 codeert en mannen met 1, dan\n\nmannen: odds = 0,727 en Logitparameter = -0,319\nvrouwen odds = 0,616 en Logitparameter = -0,485\n\nBij mannen zijn er naar verhouding MEER voldoendes dan onvoldoendes dan bij vrouwen: 0,727/0,616=1,18 (=odds ratio) 0,166\nSamenvattend\nEen odds geeft de verhouding aan tussen wel (p1) en niet (p0):\n\nals 1 dan zijn er evenveel wel als niet\nals <1 dan meer niet als wel\nals >1 dan meer wel als niet\n\nEen odds ratio geeft de verhouding aan tussen twee odds en drukt een effect uit:\n\nals 1 dan geen verband: de verhouding wel/niet is overal gelijk\neen odds ratio ligt tussen oneindig klein (kans op wel neemt af)\nen oneindig groot (kans op wel neemt toe)\n\nEen logitparameter geeft ook een effect weer:\n\nis het 0 dan geen effect\nis het negatief dan neemt de kans op wel af\nis het positief dan neemt de kans op wel toe\n\nMet logistische regressie kun je dus een binaire (dichotome) uitkomstvariabele relateren aan één of meerdere voorspelvariabelen. Er wordt gewerkt met kansverhoudingen. Niet de waarde van de afhankelijke variabele wordt gemodelleerd, maar de kans op die uitkomst."
  },
  {
    "objectID": "logitmodel.html#voorbeeld-onderwijs",
    "href": "logitmodel.html#voorbeeld-onderwijs",
    "title": "7  Logistisch model",
    "section": "7.4 Voorbeeld onderwijs",
    "text": "7.4 Voorbeeld onderwijs\nDit is een aardig voorbeeld van logistische regressie m.b.t. het onderwijs. Het voorbeeld wordt zowel uitgewerkt in R als in Excel.\nIn R behoort logistische regressie tot de familie Generalized Linear Model (GLM).\nBronnen:\n\nUCLA\nlearningtree blog\n\nVoor drie variabelen wordt bekeken of ze van invloed zijn om vanuit een bacheloropleiding toegelaten te worden tot de masteropleiding. Deze onafhankelijke voorspellende variabelen zijn:\n\ngre (Graduate Record Examination score), een gestandaardiseerde test in de V.S, welke voor de meeste masteropleidingen een vereiste is. 130 is de laagst mogelijke score en 170 de hoogste per sectie. Er zijn twee onderdelen (algemeen en vakspecifiek), dus gecombineerd loopt de score van 260 tot 340.\ngpa (Grade Point Average), een standaardmethode in de VS om te meten hoe succesvol de studie verlopen is. De score is een waarde uit het interval 0 t/m 4 punten waarbij 4 het beste is.\nrank (prestige van de bacheloropleiding), een gehele waarde uit het interval 1 (hoogste prestige) tot 4 (laagste prestige)\n\nHet al dan niet toegelaten worden tot de masteropleiding wordt bijgehouden in de afhankelijke variabele (respons variabele):\n\nadmit, binair met waarden 0 (niet toegelaten) of 1 (toegelaten).\n\nModel: \\[admit = b_0 + b_1 \\times gre + b_2 \\times gpa + b_3 \\times rank\\]\nEr is een databestand binary.csv met gegenereerde hypothetische data gemaakt.\n\n# data inlezen in dataframe\nschooldata <- read.csv(\"data/binary.csv\")\n\n\n# Overzicht van de data\nstr(schooldata)\n\n'data.frame':   400 obs. of  4 variables:\n $ admit: int  0 1 1 1 0 1 1 0 1 0 ...\n $ gre  : int  380 660 800 640 520 760 560 400 540 700 ...\n $ gpa  : num  3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ...\n $ rank : int  3 3 1 4 4 2 1 2 3 2 ...\n\nhead(schooldata)\n\n  admit gre  gpa rank\n1     0 380 3.61    3\n2     1 660 3.67    3\n3     1 800 4.00    1\n4     1 640 3.19    4\n5     0 520 2.93    4\n6     1 760 3.00    2\n\n\nDe variabelen gre, gpa worden als continue variabelen behandeld. En variabele rank kan alleen maar de gehele waarden 1 t/m 4 aannemen. Omdat rank als een categorievariabele behandeld moet worden moet deze eerst omgezet worden naar het type factor.\n|Voor vergelijking met de uitvoer in Excel kun je dat laatste misschien beter achterwege laten.\n\n# conversie variabele rank naar type factor\nschooldata$rank <- factor(schooldata$rank)\n# Statistische samenvatting van de variabelen\nsummary(schooldata)\n\n     admit            gre           gpa       rank   \n Min.   :0.000   Min.   :220   Min.   :2.26   1: 61  \n 1st Qu.:0.000   1st Qu.:520   1st Qu.:3.13   2:151  \n Median :0.000   Median :580   Median :3.40   3:121  \n Mean   :0.318   Mean   :588   Mean   :3.39   4: 67  \n 3rd Qu.:1.000   3rd Qu.:660   3rd Qu.:3.67          \n Max.   :1.000   Max.   :800   Max.   :4.00          \n\nskimr::skim(schooldata) # geeft ook standaarddeviatie en aantal NA's\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nschooldata\n\n\n\n\nNumber of rows\n\n\n400\n\n\n\n\nNumber of columns\n\n\n4\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n1\n\n\n\n\nnumeric\n\n\n3\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\nrank\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n4\n\n\n2: 151, 3: 121, 4: 67, 1: 61\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nadmit\n\n\n0\n\n\n1\n\n\n0.32\n\n\n0.47\n\n\n0.00\n\n\n0.00\n\n\n0.0\n\n\n1.00\n\n\n1\n\n\n▇▁▁▁▃\n\n\n\n\ngre\n\n\n0\n\n\n1\n\n\n587.70\n\n\n115.52\n\n\n220.00\n\n\n520.00\n\n\n580.0\n\n\n660.00\n\n\n800\n\n\n▁▂▇▇▅\n\n\n\n\ngpa\n\n\n0\n\n\n1\n\n\n3.39\n\n\n0.38\n\n\n2.26\n\n\n3.13\n\n\n3.4\n\n\n3.67\n\n\n4\n\n\n▁▃▆▇▆\n\n\n\n\n\n\n\nMaak een logistic regressiemodel met de functie glm() (generalized linear model).\n\nlogitmodel <- glm(admit ~ gre + gpa + rank, data = schooldata, family = \"binomial\")\nsummary(logitmodel)\n\n\nCall:\nglm(formula = admit ~ gre + gpa + rank, family = \"binomial\", \n    data = schooldata)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.627  -0.866  -0.639   1.149   2.079  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -3.98998    1.13995   -3.50  0.00047 ***\ngre          0.00226    0.00109    2.07  0.03847 *  \ngpa          0.80404    0.33182    2.42  0.01539 *  \nrank2       -0.67544    0.31649   -2.13  0.03283 *  \nrank3       -1.34020    0.34531   -3.88  0.00010 ***\nrank4       -1.55146    0.41783   -3.71  0.00020 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 458.52  on 394  degrees of freedom\nAIC: 470.5\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n7.4.1 Uitvoer\nDeviance Residuals\nDeze zijn een maat voor hoe goed het model past bij de data. Dit deel van de uitvoer toont de verdeling van de individuele residuen.\nCoefficients\nDit gedeelte toont de coefficienten (onder Estimate), de standaardfouten, z-waarden (soms Wald z genoemd) en de bijbehorende p-waarden. Zowel gre, gpa als de drie termen voor rank zijn statistisch significant. De logistische regressiecoefficienten geven de verandering in de log odds van de uitvoervariabele bij een toename van 1 in de voorspellende variabele.\n\nBij een toename met 1 voor gre neemt de log odds voor toelating/niet-toelating toe met 0.002264\nBij een toename met 1 voor gpa neemt de log odds voor toelating/niet-toelating toe met 0.804038\nDe indicator variabelen voor rank hebben een iets andere interpretatie. Bijvoorbeeld komend van een opleiding met rank 2 versus een opleiding met rank 1, wijzigt de log odds voor toelating met -0,675.\n\n\n\n7.4.2 Betrouwbaarheidsintervallen\nMet de functie confint() kun je betrouwbaarheidsintervallen voor de coëfficienten krijgen. Voor logistische modellen zijn deze gebaseerd op de log-likelihood functie. Wil je je gebaseerd hebben op de standaardfouten dan moet je de methode default gebruiken.\n\n# Betrouwbaarheidsintervallen basis log-likelihood\nconfint(logitmodel)\n\n                2.5 %   97.5 %\n(Intercept) -6.271620 -1.79255\ngre          0.000138  0.00444\ngpa          0.160296  1.46414\nrank2       -1.300889 -0.05675\nrank3       -2.027671 -0.67037\nrank4       -2.400027 -0.75354\n\n# Betrouwbaarheidsintervallen basis standaardfouten\nconfint.default(logitmodel)\n\n               2.5 %   97.5 %\n(Intercept) -6.22424 -1.75572\ngre          0.00012  0.00441\ngpa          0.15368  1.45439\nrank2       -1.29575 -0.05513\nrank3       -2.01699 -0.66342\nrank4       -2.37040 -0.73253\n\n\nIn het artikel staat nog meer."
  },
  {
    "objectID": "logitmodel.html#resources",
    "href": "logitmodel.html#resources",
    "title": "7  Logistisch model",
    "section": "7.5 Resources",
    "text": "7.5 Resources\n\nHow to apply Logistic Regression using Excel, Daniel Buskirk, 11-10-2017 , behandelt hetzelfde voorbeeld met admit, gre, gpa en rank\nHow to perform a Logistic Regression in R , voorbeeld met Titanic dataset\nGeneralized Linear Models in R , er zijn meerdere artikelen in deze serie\nHow do I interpret odds ratios in logistic regression? en https://stats.idre.ucla.edu/stata/faq/how-do-i-interpret-odds-ratios-in-logistic-regression/\nReal Statistics Logistic regression with Excel"
  },
  {
    "objectID": "logitmodel.html#hoe-moet-je-odds-ratios-interpreteren-in-logistische-regressie",
    "href": "logitmodel.html#hoe-moet-je-odds-ratios-interpreteren-in-logistische-regressie",
    "title": "7  Logistisch model",
    "section": "7.6 Hoe moet je odds ratios interpreteren in logistische regressie?",
    "text": "7.6 Hoe moet je odds ratios interpreteren in logistische regressie?\nBron: https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/\nEen logistisch regressiemodel stelt je in staat om een verband te leggen tussen een binaire uitkomstvariabele en een groep voorspellende variabelen. Het modelleert de logit-getransformeerde kans als een lineair verband met de voorspellende variabelen. Als\n\n\\(Y\\) is de binaire uitkomstvariabele met waarden 0/1 (mislukking/succes)\n\\(p\\) is de kans dat Y gelijk aan 1 is.\n\\(X_1, X_2, ...\\) zijn de voorspellende variabelen\n\ndan worden de waarden van de parameters \\(b_0, b_1, ...\\) geschat via de maximum likelihood methode voor de volgende vergelijking.\n\\(logit(p) = ln(odds) = ln(\\frac{p}{1-p}) = b_0 + b_1 \\times X_1 + b_2 \\times X_2 + ...\\)\nEr volgen nu een aantal voorbeelden van logistische regressiemodellen. Het databestand kan gedownload worden via https://stats.idre.ucla.edu/wp-content/uploads/2016/02/sample.csv en is ook lokaal aanwezig. Het databestand bestaat uit 200 waarnemingen en de afhankelijk variabele is hon welke aangeeft of een student in een honours class zit of niet. De kans op succes is de kans dat hon = 1.\nIn de volgende uitwerkingen ligt de nadruk op de betekenis van de regressiecoëfficienten.\n\n# data inlezen in dataframe\nmydata <- read.csv(\"data/sample.csv\")\n\n# Overzicht van de data\nstr(mydata)\n\n'data.frame':   200 obs. of  6 variables:\n $ female     : int  0 1 0 0 0 0 0 0 0 0 ...\n $ read       : int  57 68 44 63 47 44 50 34 63 57 ...\n $ write      : int  52 59 33 44 52 52 59 46 57 55 ...\n $ math       : int  41 53 54 47 57 51 42 45 54 52 ...\n $ hon        : int  0 0 0 0 0 0 0 0 0 0 ...\n $ femalexmath: int  0 53 0 0 0 0 0 0 0 0 ...\n\nhead(mydata)\n\n  female read write math hon femalexmath\n1      0   57    52   41   0           0\n2      1   68    59   53   0          53\n3      0   44    33   54   0           0\n4      0   63    44   47   0           0\n5      0   47    52   57   0           0\n6      0   44    52   51   0           0\n\n# Statistische samenvatting van de variabelen\nsummary(mydata)\n\n     female           read          write           math           hon       \n Min.   :0.000   Min.   :28.0   Min.   :31.0   Min.   :33.0   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:44.0   1st Qu.:45.8   1st Qu.:45.0   1st Qu.:0.000  \n Median :1.000   Median :50.0   Median :54.0   Median :52.0   Median :0.000  \n Mean   :0.545   Mean   :52.2   Mean   :52.8   Mean   :52.6   Mean   :0.245  \n 3rd Qu.:1.000   3rd Qu.:60.0   3rd Qu.:60.0   3rd Qu.:59.0   3rd Qu.:0.000  \n Max.   :1.000   Max.   :76.0   Max.   :67.0   Max.   :75.0   Max.   :1.000  \n  femalexmath  \n Min.   : 0.0  \n 1st Qu.: 0.0  \n Median :40.0  \n Mean   :28.6  \n 3rd Qu.:53.0  \n Max.   :72.0  \n\nskimr::skim(mydata) # toont ook standaarddeviatie\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nmydata\n\n\n\n\nNumber of rows\n\n\n200\n\n\n\n\nNumber of columns\n\n\n6\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n6\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nfemale\n\n\n0\n\n\n1\n\n\n0.54\n\n\n0.50\n\n\n0\n\n\n0.0\n\n\n1\n\n\n1\n\n\n1\n\n\n▇▁▁▁▇\n\n\n\n\nread\n\n\n0\n\n\n1\n\n\n52.23\n\n\n10.25\n\n\n28\n\n\n44.0\n\n\n50\n\n\n60\n\n\n76\n\n\n▂▇▆▆▂\n\n\n\n\nwrite\n\n\n0\n\n\n1\n\n\n52.77\n\n\n9.48\n\n\n31\n\n\n45.8\n\n\n54\n\n\n60\n\n\n67\n\n\n▂▅▆▇▇\n\n\n\n\nmath\n\n\n0\n\n\n1\n\n\n52.65\n\n\n9.37\n\n\n33\n\n\n45.0\n\n\n52\n\n\n59\n\n\n75\n\n\n▃▆▇▅▂\n\n\n\n\nhon\n\n\n0\n\n\n1\n\n\n0.24\n\n\n0.43\n\n\n0\n\n\n0.0\n\n\n0\n\n\n0\n\n\n1\n\n\n▇▁▁▁▂\n\n\n\n\nfemalexmath\n\n\n0\n\n\n1\n\n\n28.56\n\n\n27.01\n\n\n0\n\n\n0.0\n\n\n40\n\n\n53\n\n\n72\n\n\n▇▁▂▅▂\n\n\n\n\n\n\n\n\n7.6.1 Logistische regressie zonder voorspellende variabelen\nHet eenvoudigste logistische regressiemodel is een model zonder voorspellende variabelen, dus \\(logit(p) = ln(\\frac{p}{1-p}) = b_0\\)\n\nmodel1 <- glm(hon ~ 1, data = mydata, family = binomial(link = logit))\nsummary(model1)\n\n\nCall:\nglm(formula = hon ~ 1, family = binomial(link = logit), data = mydata)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n -0.75   -0.75   -0.75   -0.75    1.68  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.125      0.164   -6.85  7.6e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 222.71  on 199  degrees of freedom\nResidual deviance: 222.71  on 199  degrees of freedom\nAIC: 224.7\n\nNumber of Fisher Scoring iterations: 4\n\nlogLik(model1) #LogLikelihood\n\n'log Lik.' -111 (df=1)\n\n\nDe intercept van het model is de geschatte waarde van \\(ln(odds) = -1.1255\\). En dus \\(odds = e^{-1.1255} = 0.3245\\) en \\(p = \\frac{0.3245}{0.3245 + 1} = 0.245\\). Dus de overall kans om in een honors class te zitten (hon = 1) is 0.245.\nDeze waarde van \\(p\\) kan ook de frequentietabel van de variabele hon worden afgeleid.\n\ntbl <- table(mydata$hon)\ncbind(tbl, round(prop.table(tbl), 4))\n\n  tbl      \n0 151 0.755\n1  49 0.245\n\n\nEn in de samenvatting van de data zag je al dat hon een gemiddelde waarde van 0.245 had.\n\n\n7.6.2 Logistische regressie met 1 dichotome voorspellende variabele\nEen vervolgstap op het vorige model is door een binaire voorspellende variabele aan het model toe te voegen. In dit geval de variabele female welke de waarden 0 ( geen vrouw, dus man) of 1 (vrouw) kan hebben. Dit geeft de volgende vergelijking voor het model:\n\\[logit(p) = b_0 + b_1 \\times female\\]\nEen kruistabel van deze twee variabelen heeft het volgende resultaat.\n\nlibrary(gmodels)\nCrossTable(mydata$hon , mydata$female, digits = 1, prop.r = FALSE, \n           prop.t = FALSE, prop.chisq = FALSE, format = c(\"SPSS\"))\n\n\n   Cell Contents\n|-------------------------|\n|                   Count |\n|          Column Percent |\n|-------------------------|\n\nTotal Observations in Table:  200 \n\n             | mydata$female \n  mydata$hon |        0  |        1  | Row Total | \n-------------|-----------|-----------|-----------|\n           0 |       74  |       77  |      151  | \n             |     81.3% |     70.6% |           | \n-------------|-----------|-----------|-----------|\n           1 |       17  |       32  |       49  | \n             |     18.7% |     29.4% |           | \n-------------|-----------|-----------|-----------|\nColumn Total |       91  |      109  |      200  | \n             |     45.5% |     54.5% |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\nHieruit kun je de odds voor mannen (female = 0) en vrouwen (female = 1) berekenen.\nodds(mannen) = 17/74 = 0.2297 en LN(odds) = -1.4709\nodds(vrouwen) = 32/77 = 0.4156 en LN(odds) = -0.8781\nodds ratio vrouwen / mannen = 0.4156 / 0.2297 = 1.8093 en LN(odds ratio) = 0.5928\nDe odds voor vrouwen zijn ongeveer 81% hoger dan de odds voor mannen.\nDe lineaire relatie wordt.\n\nmodel2 <- glm(hon ~ female, data = mydata, family = binomial(link = logit))\nsummary(model2)\n\n\nCall:\nglm(formula = hon ~ female, family = binomial(link = logit), \n    data = mydata)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-0.834  -0.834  -0.643  -0.643   1.832  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.471      0.269   -5.47  4.5e-08 ***\nfemale         0.593      0.341    1.74    0.083 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 222.71  on 199  degrees of freedom\nResidual deviance: 219.61  on 198  degrees of freedom\nAIC: 223.6\n\nNumber of Fisher Scoring iterations: 4\n\nlogLik(model2) #LogLikelihood\n\n'log Lik.' -110 (df=2)\n\nexp(coef(model2)) # odds ratio\n\n(Intercept)      female \n       0.23        1.81 \n\n\nDe intercept -1.4709 is de logit voor mannen omdat dan de waarde wanneer de voorspellende variabele female gelijk aan 0 is.\nDe coëfficient voor female is 0.5928 en is de log van de odds ratio tussen de groep vrouwen en de groep mannen.\n\n\n7.6.3 Logistische regressie met 1 continue voorspellende variabele\nEen ander eenvoudig model is met 1 continue voorspellende variabele. In dit voorbeeld de variabele math voor de wiskunde scores. Dit geeft de volgende vergelijking voor het model:\n\\[logit = b_0 + b_1 \\times math\\]\n\nmodel3 <- glm(hon ~ math, data = mydata, family = binomial(link = logit))\nsummary(model3)\n\n\nCall:\nglm(formula = hon ~ math, family = binomial(link = logit), data = mydata)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-2.033  -0.678  -0.351  -0.156   2.614  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -9.7939     1.4817   -6.61  3.9e-11 ***\nmath          0.1563     0.0256    6.10  1.0e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 222.71  on 199  degrees of freedom\nResidual deviance: 167.07  on 198  degrees of freedom\nAIC: 171.1\n\nNumber of Fisher Scoring iterations: 5\n\nlogLik(model3) #LogLikelihood\n\n'log Lik.' -83.5 (df=2)\n\nexp(coef(model3)) # odds ratio\n\n(Intercept)        math \n   5.58e-05    1.17e+00 \n\n\nIn dit geval geeft intercept -9.79394 de log odds voor een student in de honor class met een math score van nul. Anders gezegd, de odds van een student in de honors class met een math score nul is \\(e{-9.79394} = 0.0000559\\). Dit is een erg lage odds. Maar in de dataset is 33 de minimumwaarde van math. Dus de intercept correspondeert hier met een hypothetische waarde voor math van nul.\nHoe moet je de coefficient van math interpreteren? Intercept en coefficient geven de volgende vergelijking:\n\\[LN(\\frac{p}{1-p}) = -9.79394 + 0.15634 \\times math\\]\nVoor bijvoorbeeld math =54 geeft dit\n\\[LN(\\frac{p}{1-p}) = -9.79394 + 0.15634 \\times 54 = -1.35158\\]\nEn voor math score die 1 eenheid groter (55) is\n\\[LN(\\frac{p}{1-p}) = -9.79394 + 0.15634 \\times55 = -1.19524\\]\neen waarde die \\(-1.19524 - -1.35158 = 0.15634\\) groter is. De coefficient voor math is dus het verschil in log odds.\nEn \\(e^{0.15634} = 1.169224\\) wat aangeeft dat verhoging van de math score met 1 eenheid een ongeveer 17% hogere odds geeft.\n\n\n7.6.4 Logistische regressie meet meerdere voorspellende variabelen zonder interacties\nIn het algemeen kun je meerdere voorspellende variabelen in een logistisch regressiemodel hebben.\n\\[logit(p) = ln(\\frac{p}{1-p}) = ln(odds) = b_0 + b_1 \\times X_1 + b_2 \\times X_2 + ... + b_k \\times X_k\\]\nElke berekende waarde voor een coëfficient geeft de verwachte verandering in de log odds om in de honour klas te komen bij een verhoging met 1 voor de bijbehorende voorspellende variabele, waarbij de andere voorspellende variabelen constant gehouden worden. Een voorbeeld:\n\\[logit = b_0 + b_1 \\times math + b_2 \\times female + b_3 \\times read\\]\n\nmodel4 <- glm(hon ~ math + female + read, data = mydata, family = binomial(link = logit))\nsummary(model4)\n\n\nCall:\nglm(formula = hon ~ math + female + read, family = binomial(link = logit), \n    data = mydata)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.831  -0.633  -0.330  -0.126   2.390  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -11.7702     1.7107   -6.88  6.0e-12 ***\nmath          0.1230     0.0313    3.93  8.4e-05 ***\nfemale        0.9799     0.4216    2.32    0.020 *  \nread          0.0591     0.0266    2.22    0.026 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 222.71  on 199  degrees of freedom\nResidual deviance: 156.17  on 196  degrees of freedom\nAIC: 164.2\n\nNumber of Fisher Scoring iterations: 5\n\nlogLik(model4) #LogLikelihood\n\n'log Lik.' -78.1 (df=4)\n\n\nJe kunt de resultaten als volgt interpreteren. Wanneer de waarden voor math en read op een vaste waarde gehouden worden, dan wordt de odds voor vrouwen (female=1) om in een honor class te komen tegenover de odds voor mannen (female=0) \\(e^{0.97995} = 2.664323\\). In procenten is de odds voor vrouwen 166% hoger dan de odds voor mannen. Analoge redeneringen kun je voor de andere coefficienten maken.\n\n\n7.6.5 Logistische regressie met een interactieterm voor 2 voorspellende variabelen\nWanneer een model interactieterm (en) van twee voorspellende variabelen heeft, probeert het te beschrijven hoe het effect van een voorspellende variabele afhangt van de andere voorspellende variabele. De interpretatie van de regressiecoëfficiënten wordt dan wat ingewikkelder.\nEen voorbeeld.\n\\[logit = b_0 + b_1 \\times female + b_2 \\times math + b_3 \\times female \\times math\\]\n\nmodel5 <- glm(hon ~ female + math + female*math, data = mydata, family = binomial(link = logit))\nsummary(model5)\n\n\nCall:\nglm(formula = hon ~ female + math + female * math, family = binomial(link = logit), \n    data = mydata)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.762  -0.673  -0.342  -0.145   2.691  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -8.7458     2.1291   -4.11    4e-05 ***\nfemale       -2.8999     3.0942   -0.94  0.34866    \nmath          0.1294     0.0359    3.61  0.00031 ***\nfemale:math   0.0670     0.0535    1.25  0.21014    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 222.71  on 199  degrees of freedom\nResidual deviance: 159.77  on 196  degrees of freedom\nAIC: 167.8\n\nNumber of Fisher Scoring iterations: 5\n\nlogLik(model5) #LogLikelihood\n\n'log Lik.' -79.9 (df=4)\n\n\nVanwege de interactieterm female met math, kun je niet langer een interpretatie geven van het effect van de variabele female terwijl je alle andere variabelen onveranderd houdt. Het slaat namelijk nergens op om math en female * math op een bepaalde waarde vast te zetten en toch toestaan dat female verandert van 0 in 1.\nIn dit eenvoudige voorbeeld van een interactie tussen een binaire variabele en een continue variabele heb je in feite twee vergelijkingen, eentje voor mannen en eentje voor vrouwen.\n\nMannen (female =0): \\(logit(p) = b_0 + b_2 \\times math\\)\nvrouwen (female=1): \\(logit(p) = (b_0 + b_1) + (b_2 + b_3) \\times math\\)\n\nNu kan de de uitvoer van de logistische regressie naar deze twee vergelijkingen overgebracht worden. Zo kun je zeggen dat de coefficient voor math het effect van math is wanneer female=0. Meer expliciet kun je zeggen dat voor mannen een verhoging van math met 1, een verandering van log odds met 0.13 geeft.\nEn voor de vrouwen geeft een verhoging van math met 1, een verandering van log odds met (.13 + .067) = 0.197. In termen van odds kun je zeggen dat voor mannen is \\(e^{0.13} = 1.14\\) voor een verhoging van de math score met 1. En voor vrouwen is dit \\(e^{0.197}=1.22\\). En de odds ratio (vrouwen -mannen) is de e-macht van de coefficient voor interactieterm **female*math**, \\(\\frac{1.22}{1.14}= e^{0.067}=1.07\\).\n\n\n\n\n1. ‘Fisher’s Exact Test’, Wikipedia, (2022).\n\n\n2. ‘Unusual Observations - Outlier, Leverage, and Influential Points’, https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A., ‘Statistiek om mee te werken’, Third dr., Stenfert Kroese, (1991).\n\n\n4. Chouldechova, A., ‘Regression Diagnostic Plots’, https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, ‘R Linear Regression Tutorial: Lm Function in R with Code Examples’, https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., ‘Finding a Suitable Linear Model for Ozone Prediction’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M., ‘Interpreting Linear Prediction Models’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, ‘Beginners Guide to Regression Analysis and Plot Interpretations’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, ‘Practical Guide to Logistic Regression Analysis in R’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A., ‘Logistic Regression Essentials in R’, 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., ‘Machine Learning Essentials: Practical Guide in R’, 1st edition dr., (2018).\n\n\n12. University of Virginia, ‘Understanding Diagnostic Plots for Linear Regression Analysis | University of Virginia Library Research Data Services + Sciences’, https://data.library.virginia.edu/diagnostic-plots/."
  },
  {
    "objectID": "voorbeelden.html#inkomen-ervaring",
    "href": "voorbeelden.html#inkomen-ervaring",
    "title": "8  Voorbeelden",
    "section": "8.1 inkomen-ervaring",
    "text": "8.1 inkomen-ervaring\nIn dit voorbeeld [3, pag. 303] wordt het verband onderzocht tussen het inkomen (x €1000) en het aantal jaren ervaring van een groep managers.\n\nmydata <- data.frame(ervaring = c(5, 10, 8, 15, 12), \n                     inkomen = c(60, 80, 90, 120, 100))\nmydata\n\n  ervaring inkomen\n1        5      60\n2       10      80\n3        8      90\n4       15     120\n5       12     100\n\n\nAllereerst wordt een spreidingsdiagram gemaakt.\n\nplot(inkomen ~ ervaring, data = mydata,\n     xlab = \"Ervaring (jaren)\", ylab = \"Inkomen (x €1000)\")\n\n\n\n\nFiguur 8.1: Relatie tussen inkomen en aantal jaren ervaring.\n\n\n\n\nOpstellen lineair model: \\(inkomen = b_0 + b1 * ervaring\\)\n\nmodel <- lm(inkomen ~ ervaring, data = mydata)\nsummary(model)\n\n\nCall:\nlm(formula = inkomen ~ ervaring, data = mydata)\n\nResiduals:\n     1      2      3      4      5 \n -2.41 -10.00  11.03   2.41  -1.03 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    34.83      12.26    2.84    0.066 .\nervaring        5.52       1.16    4.75    0.018 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.84 on 3 degrees of freedom\nMultiple R-squared:  0.883, Adjusted R-squared:  0.844 \nF-statistic: 22.6 on 1 and 3 DF,  p-value: 0.0177\n\n\nDe regressievergelijking wordt dus \\(inkomen = 34.828 + 5.517*ervaring\\)\nDe geschatte waarden (berekend volgens de regressievergelijking) en residuen zijn\n\nmodel$fitted.values\n\n    1     2     3     4     5 \n 62.4  90.0  79.0 117.6 101.0 \n\nmodel$residuals\n\n     1      2      3      4      5 \n -2.41 -10.00  11.03   2.41  -1.03 \n\n\n\nplot(model$residuals, \n     main = \"Residuen model Inkomen-Ervaring\", ylab = \"Residuen\")\nabline(h = mean(model$residuals))\n\n\n\n\nFiguur 8.2: Residuen bij lineair model Inkomen-Ervaring.\n\n\n\n\nJe kunt zien dat de residuen willekeurig verdeeld zijn en het gemiddelde nagenoeg 0 is."
  },
  {
    "objectID": "voorbeelden.html#kinderlengte",
    "href": "voorbeelden.html#kinderlengte",
    "title": "8  Voorbeelden",
    "section": "8.2 kinderlengte",
    "text": "8.2 kinderlengte\nVan 12 kinderen is de leeftijd (jaren) en lengte (cm bepaald). Maak een model waarbij de lengte verklaard wordt uit de leeftijd.\n\nkind <- data.frame(leeftijd = seq(18,29),\n    lengte = c(76.1,77.0,78.1,78.2,78.8,79.7,79.9,81.1,81.2,81.8,82.8,83.5))\n# spreidingsdiagram\nplot(lengte ~ leeftijd, data = kind,\n     xlab = \"Leeftijd (maanden)\", ylab = \"Lengte (cm)\")\n\n\n\n\nFiguur 8.3: Relatie tussen lengte en leeftijd van 12 kinderen.\n\n\n\n\nHet model wordt \\(\\text{lengte} = b_0 + b_1\\times\\text{leeftijd}\\)\n\nkind.model <- lm(lengte ~ leeftijd, data = kind)\nsummary(kind.model)\n\n\nCall:\nlm(formula = lengte ~ leeftijd, data = kind)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2724 -0.2425 -0.0276  0.1601  0.4724 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  64.9283     0.5084   127.7  < 2e-16 ***\nleeftijd      0.6350     0.0214    29.7  4.4e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.256 on 10 degrees of freedom\nMultiple R-squared:  0.989, Adjusted R-squared:  0.988 \nF-statistic:  880 on 1 and 10 DF,  p-value: 4.43e-11\n\n\nDe regressievergelijking wordt dus \\(\\text{lengte} = 64.9283 + 0.635\\times\\text{leeftijd}\\).\nHet model wordt nu uitgebreid door ook het aantal broers en zussen (variabele broerzus) in de formule op te nemen. het model wordt dan\n\\(\\text{lengte} = b_0 + b_1\\times\\text{leeftijd} + b_2\\times\\text{broerzus}\\)\n\nlibrary(dplyr)\nkind2 <- kind %>% mutate(broerzus = c(1, 2, 3, 2, 0, 1, 5, 0, 1, 4, 1, 5))\nkind2.model <- lm(lengte ~ leeftijd + broerzus, data = kind2)\nsummary(kind2.model)\n\n\nCall:\nlm(formula = lengte ~ leeftijd + broerzus, data = kind2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2630 -0.2246 -0.0202  0.1610  0.4975 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  64.9055     0.5353  121.26  9.0e-16 ***\nleeftijd      0.6375     0.0234   27.25  5.9e-10 ***\nbroerzus     -0.0177     0.0473   -0.37     0.72    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.268 on 9 degrees of freedom\nMultiple R-squared:  0.989, Adjusted R-squared:  0.986 \nF-statistic:  402 on 2 and 9 DF,  p-value: 1.58e-09\n\n\nDe coëfficient voor de nieuwe verklarende variabele broerzus is negatief. Voor elke broer/zus meer neemt de lengte met 0.02 cm af.\nVoor leeftijd is de p-waarde significant, deze is dus een goede voorspeller voor lengte. Maar de p-waarde voor broerzus (het aantal broers en zussen) is niet significant, deze variabele past dus niet goed in het model."
  },
  {
    "objectID": "voorbeelden.html#sparen",
    "href": "voorbeelden.html#sparen",
    "title": "8  Voorbeelden",
    "section": "8.3 sparen",
    "text": "8.3 sparen\n\nsparen <- read.csv(\"data/inkomen-spaargeld.csv\")\nhead(sparen)\n\n  Jaar Spaargeld Inkomen\n1 1974     12298   64968\n2 1975     14196   69233\n3 1976     17320   73824\n4 1977     19995   85267\n5 1978     23601   91507\n6 1979     24213   99632\n\nplot(Spaargeld ~Inkomen, data = sparen)\n\n\n\n\nFiguur 8.4: Voorbeeld sparen met relatie tussen Spaargeld en Inkomen.\n\n\n\n\n\nuitkomstvariabele: Spaargeld\nvoorspelvariabele: Inkomen\n\nAlgemene vorm regressievergelijking: \\(Spaargeld = b_0 + b_1 * Inkomen\\)\nEen lineaire relatie is in de grafiek duidelijk zichtbaar.\n\nmodel.sparen <- lm(Spaargeld ~ Inkomen, data = sparen)\nsummary(model.sparen)\n\n\nCall:\nlm(formula = Spaargeld ~ Inkomen, data = sparen)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13036  -4959   -317   5368  16969 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.10e+04   2.46e+03   -4.47  0.00024 ***\nInkomen      2.97e-01   6.01e-03   49.40  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7330 on 20 degrees of freedom\nMultiple R-squared:  0.992, Adjusted R-squared:  0.991 \nF-statistic: 2.44e+03 on 1 and 20 DF,  p-value: <2e-16\n\n\nDe vergelijking van de regressielijn wordt dan \\(Spaargeld = -10990 + 0.297 * Inkomen\\)\nDe p-waarde voor Inkomen is significant (< 0.05) en dus is de variabele significant voor het voorspellen van Spaargeld.\nAdjusted R-squared: 0.9915 houdt in dat het model 99% van de variatie in de gegevens verklaart.\nDiagnostische grafieken\n\nplot(model.sparen)\n\n\n\n\n\n\n\n(a) Residuals - Fitted values\n\n\n\n\n\n\n\n(b) Normal Q-Q\n\n\n\n\n\n\n\n\n\n(c) Scale-Location\n\n\n\n\n\n\n\n(d) Residuals - Leverage\n\n\n\n\nFiguur 8.5: Diagnostische grafieken lineair model Sparen-Inkomen.\n\n\n\n\nDe grafiek Residuals-Fitted laat een gebogen patroon zien. Dit zou kunnen betekenen dat je een beter model kunt krijgen door een kwadratische term aan het model toe te voegen.\nDe grafiek Residuals-Leverage laat zien dat waarneming 22 een grote invloed op het model heeft. Dit zou kunnen betekenen dat je waarneming 22 uit moet sluiten voor het maken van een model.\n\nJe hebt nu drie keuzes:\n\nKeur de opname van waarneming 22 goed en houd het model zoals het is.\nMaak een nieuw model en neem daarin een kwadratische term op.\nSluit waarneming 22 uit en maak een nieuw model.\n\nDe laatste twee keuzes worden verder geanalyseerd.\n\nModel met kwadratische term\nRegressievergelijking: \\(Spaargeld = b_0 + b_1*Inkomen + b_2*Inkomen^2\\)\nOm in R een variabele \\(x^2\\) te maken moet je deze definieren met I(x^2).\nEen alternatief is met gebruik van de polynoomfunctie poly(x, degree=2, raw = TRUE). Deze laatste heeft als voordeel dat je eenvoudiger hogere graads polynomen kunt maken.\n\nmodel2.sparen <-  lm(Spaargeld ~ Inkomen + I(Inkomen^2), data = sparen)\n# model2.sparen <-  lm(Spaargeld ~ poly(Inkomen, degree=2, raw = TRUE), data = sparen)\nsummary(model2.sparen)\n\n\nCall:\nlm(formula = Spaargeld ~ Inkomen + I(Inkomen^2), data = sparen)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10304  -2027    252   2149  10353 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.94e+02   2.48e+03   -0.24     0.81    \nInkomen       2.19e-01   1.49e-02   14.64  8.4e-12 ***\nI(Inkomen^2)  8.56e-08   1.58e-08    5.42  3.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4710 on 19 degrees of freedom\nMultiple R-squared:  0.997, Adjusted R-squared:  0.996 \nF-statistic: 2.97e+03 on 2 and 19 DF,  p-value: <2e-16\n\n\n\nplot(model2.sparen)\n\n\n\n\n\n\n\n(a) Residuals - Fitted values\n\n\n\n\n\n\n\n(b) Normal Q-Q\n\n\n\n\n\n\n\n\n\n(c) Scale-Location\n\n\n\n\n\n\n\n(d) Residuals - Leverage\n\n\n\n\nFiguur 8.6: Diagnostische grafieken voor het model met een kwadratische term.\n\n\n\nDe diagnostische grafieken zijn nu veel beter. Residuen zijn bijna horizontaal en goed verspreid. Verspreiding is bijna uniform en geen enkele punt heeft een teveel aan leverage. De Q-Q grafiek laat echter zien dat enkele punten niet op de Normaal lijn staan, maar dat kan acceptabel zijn.\n\n\nModel met uitsluiting waarneming\nWaarneming 22 wordt uitgesloten.\n\nsparen21 <- sparen[1:21,]\nmodel.sparen21 <-  lm(Spaargeld ~ Inkomen, data = sparen21)\nsummary(model.sparen21)\n\n\nCall:\nlm(formula = Spaargeld ~ Inkomen, data = sparen21)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8601  -3585  -1088   4294  15186 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.69e+03   2.06e+03   -4.21  0.00047 ***\nInkomen      2.86e-01   5.69e-03   50.25  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5840 on 19 degrees of freedom\nMultiple R-squared:  0.993, Adjusted R-squared:  0.992 \nF-statistic: 2.53e+03 on 1 and 19 DF,  p-value: <2e-16\n\n\n\nplot(model.sparen21)\n\n\n\n\n\n\n\n(a) Residuals - Fitted values\n\n\n\n\n\n\n\n(b) Normal Q-Q\n\n\n\n\n\n\n\n\n\n(c) Scale-Location\n\n\n\n\n\n\n\n(d) Residuals - Leverage\n\n\n\n\nFiguur 8.7: Diagnostische grafieken voor het model met uitsluiting waarneming.\n\n\n\nAlle diagnostische grafieken lijken niet goed, ze zijn zelfs slechter dan het oorspronkelijke model.\n\n\nConclusie\nTot zo ver komt het model met de kwadratische term het beste voort uit de analyse van de diagnostische grafieken."
  },
  {
    "objectID": "voorbeelden.html#druk-temperatuur",
    "href": "voorbeelden.html#druk-temperatuur",
    "title": "8  Voorbeelden",
    "section": "8.4 druk-temperatuur",
    "text": "8.4 druk-temperatuur\nIn dit voorbeeld wordt de relatie tussen druk en temperatuur onderzocht.\n\ndruktemp <- data.frame(temp = seq(from = 10, to = 100, by = 10), \n                       druk = c(18.5, 24.9, 30.3, 43.7, 80.1, 118.5, 169.9, 228.3, 302.7,385.1))\n# lineair model\nmodel1 <- lm(druk ~ temp, data = druktemp)\n# Spreidingsdiagram met regressielijn\nplot(druk ~ temp, data = druktemp)\nabline(reg = model1)\nsummary(model1) # samenvatting model\n\n\nCall:\nlm(formula = druk ~ temp, data = druktemp)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -41.9  -34.7  -10.9   24.7   63.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -81.50      29.14   -2.80    0.023 *  \ntemp            4.03       0.47    8.58  2.6e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.7 on 8 degrees of freedom\nMultiple R-squared:  0.902, Adjusted R-squared:  0.89 \nF-statistic: 73.7 on 1 and 8 DF,  p-value: 2.62e-05\n\n\n\n\n\nFiguur 8.8: Relatie Druk - Temperatuur\n\n\n\n\nBeide \\(R^2\\) waarden zijn goed. Idealiter zouden de residuen in een grafiek er willekeurig uit moeten zien.\n\nplot(model1$residuals, pch = 16, col = \"red\", \n     main = \"Residuen Druk~Temperatuur\", ylab = \"Residuen\")\n\n\n\n\nFiguur 8.9: Residuen bij het lineair model druk-temperatuur.\n\n\n\n\nJe ziet dat het gemiddelde wel ongeveer nul is, maar de waarden vertonen geen willekeurig karakter, het diagram ziet er als een parabool uit. Dat betekent dat er misschien een verborgen patroon is dat niet door het lineaire model wordt meegenomen. wat je nu kunt doen is de variabele transformeren of een term toevoegen, bijv. \\(x^2, x^3, ln(x), ln(x+1), \\sqrt(x), \\frac{1}{x}, exp(x)\\).\n\n\n\n\n\n\nOpmerking\n\n\n\nVanwege het paraboolkarakter van de residuen wordt een kwadratische term toegevoegd.\n\n\nOm verwarring te voorkomen wordt de hoofdletter I voor de transformatie gezet.\n\nmodel2 <- lm(druk ~ temp + I(temp^2),  data = druktemp)\nsummary(model2)\n\n\nCall:\nlm(formula = druk ~ temp + I(temp^2), data = druktemp)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.605 -1.633  0.555  1.180  4.827 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.75000    3.61559    9.33  3.4e-05 ***\ntemp        -1.73159    0.15100  -11.47  8.6e-06 ***\nI(temp^2)    0.05239    0.00134   39.16  1.8e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.07 on 7 degrees of freedom\nMultiple R-squared:     1,  Adjusted R-squared:  0.999 \nF-statistic: 7.86e+03 on 2 and 7 DF,  p-value: 1.86e-12\n\n\nDe p-waarden zijn nu veel kleiner geworden, De \\(R^2\\)-waarden zijn bijna gelijk aan 1 geworden. Dus het model is behoorlijk verbeterd. Van dit nieuwe model wordt nu ook een grafiek van de residuen gemaakt.\n\nplot(model2$residuals, pch = 16, col = \"red\", \n     main = \"Druk~Temperatuur model met kwadratische term\", ylab = \"Residuen\")\n\n\n\n\nFiguur 8.10: Residuen bij lineair model Lengte~Leeftijd waaraan een kwadratische term is toegevoegd.\n\n\n\n\nEr is nu geen patroon te zien, de residuen zijn behoorlijk willekeurig. Het model is nu goed."
  },
  {
    "objectID": "voorbeelden.html#aandelenindex",
    "href": "voorbeelden.html#aandelenindex",
    "title": "8  Voorbeelden",
    "section": "8.5 aandelenindex",
    "text": "8.5 aandelenindex\nDe dataset economiedata.csv bevat een gefingeerde verzameling economische gegevens met de variabelen Jaar, Maand, Rentepercentage, Werkloosheidspercentage en Aandelenindex. Maak een lineair model van Aandelenindex op basis van Rentepercentage en Werkloosheidspercentage.\n\nuitkomstvariabele\n\nAandelenindex\n\nvoorspelvariabelen\n\nRentepercentage\nWerkloosheidspercentage\n\n\nData\n\neconomiedata <- read.csv(\"data/economiedata.csv\")\nhead(economiedata)\n\n  Jaar Maand Rentepercentage Werkloosheidspercentage Aandelenindex\n1 2016     1            1.75                     6.1           719\n2 2016     2            1.75                     6.2           704\n3 2016     3            1.75                     6.2           822\n4 2016     4            1.75                     5.9           876\n5 2016     5            1.75                     6.1           866\n6 2016     6            1.75                     6.1           884\n\n\nAllereerst wordt visueel beoordeeld of er een (lineaire) relatie is tussen de afhankelijke variabele en elk van de onafhankelijke variabelen afzonderlijk.\n\nplot(Aandelenindex ~ Rentepercentage, data = economiedata)\n\n\n\n\nFiguur 8.11: Relatie tussen aandelenindex en rentepercentage.\n\n\n\n\nEr is een positief lineair verband te zien, als het Rentepercentage omhoog gaat, gaat ook de Aandelenindex omhoog.\n\nplot(Aandelenindex ~ Werkloosheidspercentage, data = economiedata)\n\n\n\n\nRelatie tussen aandelenindex en werkloosheidspercentage.\n\n\n\n\nEr is een negatief lineair verband te zien, als het Werkloosheidspercentage omhoog gaat, gaat de Aandelenindex omlaag.\nModel\n\nmodel <- lm(Aandelenindex ~ Rentepercentage + Werkloosheidspercentage, \n            data = economiedata)\nsummary(model, correlation = TRUE) # met extra een matrix van de correlaties\n\n\nCall:\nlm(formula = Aandelenindex ~ Rentepercentage + Werkloosheidspercentage, \n    data = economiedata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-158.21  -41.67   -6.25   57.74  118.81 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)   \n(Intercept)                 1798        899    2.00   0.0586 . \nRentepercentage              346        111    3.10   0.0054 **\nWerkloosheidspercentage     -250        118   -2.12   0.0460 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 70.6 on 21 degrees of freedom\nMultiple R-squared:  0.898, Adjusted R-squared:  0.888 \nF-statistic: 92.1 on 2 and 21 DF,  p-value: 4.04e-11\n\nCorrelation of Coefficients:\n                        (Intercept) Rentepercentage\nRentepercentage         -0.96                      \nWerkloosheidspercentage -1.00        0.93"
  },
  {
    "objectID": "voorbeelden.html#luchtkwaliteit",
    "href": "voorbeelden.html#luchtkwaliteit",
    "title": "8  Voorbeelden",
    "section": "8.6 luchtkwaliteit",
    "text": "8.6 luchtkwaliteit\nDit voorbeeld is geïnspireerd door een artikel in Data Science Blog [7] en gebruikt de in package datasets aanwezige gegevensverzameling airquality. Deze dataset bevat metingen van de luchtkwaliteit in New York van mei tot september 1973. Er zijn 153 waarnemingen van 6 variabelen:\n\nOzone - gemiddelde hoeveelheid ozon in parts per billion (ppb)\nSolar.R - zonnestraling in Langleys (lang)\nWind - gemiddelde windsnelheid in miles per hour (mph)\nTemp - maximale dagelijkse temperatuur in graden Fahrenheit op La Guardia Airport.\nMonth - maand (nummer 1-12)\nDay - dag ( nummer 1-31)\n\nVoor de analyse kunnen maand en dag worden weggelaten. Verder worden de waarnemingen met ontbrekende waarden verwijderd.\n\nlibrary(dplyr)\ngoed <- complete.cases(airquality)\naqdata <- airquality[goed,] %>%\n    select(Ozone, Solar.R, Wind, Temp)\nstr(aqdata)\n\n'data.frame':   111 obs. of  4 variables:\n $ Ozone  : int  41 36 12 18 23 19 8 16 11 14 ...\n $ Solar.R: int  190 118 149 313 299 99 19 256 290 274 ...\n $ Wind   : num  7.4 8 12.6 11.5 8.6 13.8 20.1 9.7 9.2 10.9 ...\n $ Temp   : int  67 72 74 62 65 59 61 69 66 68 ...\n\n\nOnderzoeksvraag\nKun je het ozonniveau voorspellen op basis van zonnestraling, windsnelheid en temperatuur?\nVerkenning data\nOm te zien of de aannames van het lineaire model geschikt zijn voor de beschikbare gegevens, worden de correlaties tussen de variabelen bekeken: een scatterplot en de correlatiecoëfficienten.\n\nplot(aqdata)\ncor(aqdata)\n\n         Ozone Solar.R   Wind   Temp\nOzone    1.000   0.348 -0.612  0.699\nSolar.R  0.348   1.000 -0.127  0.294\nWind    -0.612  -0.127  1.000 -0.497\nTemp     0.699   0.294 -0.497  1.000\n\n\n\n\n\nFiguur 8.12: Correlaties tussen de variabelen.\n\n\n\n\nConclusies\n\nOzone heeft een negatieve correlatie met Wind.\nOzone heeft een positievecorrelatie met Temp.\n\nHet zou dus mogelijk moeten zijn om een lineair model te maken dat het ozonniveau voorspelt met de kemerken.\nSplitsing in training en test set\n\nset.seed(123)\ntrainRijnummers <- createDataPartition(y = aqdata$Ozone, p=0.8, list = FALSE)\ntrainData <- aqdata[trainRijnummers, ]\ntestData <- aqdata[-trainRijnummers, ]\n\nLineair model\n\nmodel <- lm(Ozone ~ Solar.R + Wind + Temp, data = trainData)\nmodel.samenvatting <- summary(model)\nmodel.samenvatting\n\n\nCall:\nlm(formula = Ozone ~ Solar.R + Wind + Temp, data = trainData)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-39.83 -14.00  -3.95   9.98  95.80 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -66.0175    27.4527   -2.40    0.018 *  \nSolar.R       0.0739     0.0284    2.60    0.011 *  \nWind         -3.3171     0.7500   -4.42  2.8e-05 ***\nTemp          1.6285     0.3065    5.31  8.4e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.9 on 86 degrees of freedom\nMultiple R-squared:  0.596, Adjusted R-squared:  0.582 \nF-statistic: 42.3 on 3 and 86 DF,  p-value: <2e-16\n\n\nDe mediaan van de residuen is -3.952, wat suggereert dat het model over het algemeen wat hogere ozonwaarden voorspelt dan waargenomen. De grote maximumwaarde geeft echter aan dat sommige uitbijtervoorspellingen ook veel te laag zijn\n\n\n\n\n\n\nOpmerking\n\n\n\nResidu = waargenomen - verwacht. Ofwel verwacht = waargenomen - residu. Omdat de mediaanwaarde ongeveer -4 is, is de verwachte waarde = waargenomen waarde + 4.\n\n\nResiduen\nKijken naar getallen als mediaan en maximum voor de residuen kan een beetje abstract zijn. Wanneer je de verwachte waarden uitzet tegen de waargenomen waarden krijg je een betere indruk van de kwaliteit van het model.model\n\nresiduen <- model$residuals  # alternatief: residuals(model)\nwaarnemingen <- trainData$Ozone\nverwachtingen <- model$fitted.values\nwaardenbereik <- range(verwachtingen, waarnemingen)\nplot(verwachtingen ~ waarnemingen,\n     xlim = waardenbereik, ylim = waardenbereik)\nabline(0, 1, col = \"red\")\nsegments(waarnemingen, verwachtingen, waarnemingen, verwachtingen+residuen)\n\n\n\n\nFiguur 8.13: Verwachte waarden voor trainData op basis van de regressielijn vs de waargenomen waarden, met daarbij de residuen als vertikale lijnstukjes.\n\n\n\n\n\n\n\n\n\n\nIn de grafiek is te zien dat er ook negatieve ozonwaarden voorspeld worden. Dit kan natuurlijk niet omdat ozonconcentraties niet onder 0 kunnen komen. In een later stadium zal daarom een beter model gemaakt moeten worden waarbij dat niet gebeurt,\n\n\n\nCoëfficienten\n\nmodel$coefficients  # alternatief: coefficients(model)\n\n(Intercept)     Solar.R        Wind        Temp \n   -66.0175      0.0739     -3.3171      1.6285 \n\n\n\nDe intercept van het model heeft een vrij lage waarde. Dit is de waarde die het model voorspelt wanneer de waarden van alle voorspelvariabelen nul zijn.\nDe lage coëfficiënt voor Solar.R geeft aan dat zonnestraling geen belangrijke rol speelt bij het voorspellen van ozonniveaus, wat niet als een verrassing komt omdat het in de verkennende analyse geen grote correlatie met het ozonniveau liet zien.\nDe coëfficiënt voor Temp geeft aan dat de ozonniveaus hoog zijn als de temperatuur hoog is (er vormt zich dan sneller ozon).\nDe coëfficiënt voor Wind geeft aan dat de ozonniveaus laag zullen zijn voor hogere windsnelheden (de ozon zal dan worden weggeblazen).\n\nDe overige waarden die bij de coëfficiënten horen, geven informatie over de statistische zekerheid van de schattingen.\n\nmodel.samenvatting$coefficients\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -66.0175    27.4527   -2.40 1.83e-02\nSolar.R       0.0739     0.0284    2.60 1.09e-02\nWind         -3.3171     0.7500   -4.42 2.84e-05\nTemp          1.6285     0.3065    5.31 8.37e-07\n\n\n\n\n\n\n\n\nTip\n\n\n\nVoor een verbeterd model zie [6]\n\n\n\n\n\n\n1. ‘Fisher’s Exact Test’, Wikipedia, (2022).\n\n\n2. ‘Unusual Observations - Outlier, Leverage, and Influential Points’, https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A., ‘Statistiek om mee te werken’, Third dr., Stenfert Kroese, (1991).\n\n\n4. Chouldechova, A., ‘Regression Diagnostic Plots’, https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, ‘R Linear Regression Tutorial: Lm Function in R with Code Examples’, https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., ‘Finding a Suitable Linear Model for Ozone Prediction’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M., ‘Interpreting Linear Prediction Models’, Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, ‘Beginners Guide to Regression Analysis and Plot Interpretations’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, ‘Practical Guide to Logistic Regression Analysis in R’, https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A., ‘Logistic Regression Essentials in R’, 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., ‘Machine Learning Essentials: Practical Guide in R’, 1st edition dr., (2018).\n\n\n12. University of Virginia, ‘Understanding Diagnostic Plots for Linear Regression Analysis | University of Virginia Library Research Data Services + Sciences’, https://data.library.virginia.edu/diagnostic-plots/."
  },
  {
    "objectID": "referenties.html",
    "href": "referenties.html",
    "title": "Referenties",
    "section": "",
    "text": "1. “Fisher’s\nexact test,” Wikipedia, (2022).\n\n\n2. “Unusual Observations - Outlier,\nLeverage, and Influential Points,” https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points.\n\n\n3. Buijs, A.,\n“Statistiek om mee te werken,” Third ed.,\nStenfert Kroese, (1991).\n\n\n4. Chouldechova, A.,\n“Regression diagnostic plots,” https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html.\n\n\n5. Datacamp, “R\nLinear Regression Tutorial: Lm Function in\nR with Code Examples,” https://www.datacamp.com/tutorial/linear-regression-R.\n\n\n6. Döring, M., “Finding a\nSuitable Linear Model for Ozone\nPrediction,” Data Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/.\n\n\n7. Döring, M.,\n“Interpreting Linear Prediction Models,”\nData Science Blog, 2018. https://www.datascienceblog.net/post/machine-learning/linear_models/.\n\n\n8. Hackerearth, “Beginners\nGuide to Regression Analysis and Plot\nInterpretations,” https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-guide-regression-analysis-plot-interpretations/tutorial/.\n\n\n9. Hackerearth, “Practical\nGuide to Logistic Regression Analysis in\nR,” https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/logistic-regression-analysis-r/tutorial/.\n\n\n10. Kassambara, A.,\n“Logistic Regression Essentials in\nR,” 2018. http://www.sthda.com/english/articles/36-classification-methods-essentials/151-logistic-regression-essentials-in-r/.\n\n\n11. Kassambara, A., “Machine\nLearning Essentials: Practical Guide in\nR,” 1st edition ed., (2018).\n\n\n12. University of Virginia,\n“Understanding Diagnostic Plots for Linear\nRegression Analysis | University of Virginia\nLibrary Research Data Services + Sciences,”\nhttps://data.library.virginia.edu/diagnostic-plots/."
  }
]